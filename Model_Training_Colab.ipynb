{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6068df59",
   "metadata": {},
   "source": [
    "# ğŸ” URL Security Classifier â€” Model Training\n",
    "\n",
    "## Ensemble: XGBoost + DistilBERT (Binary Classification)\n",
    "\n",
    "**Goal**: Train a high-accuracy model to classify URLs as **Safe** or **Malicious**\n",
    "\n",
    "**Architecture**:\n",
    "1. **XGBoost** â€” 100+ hand-crafted URL features â†’ explainable, fast inference\n",
    "2. **DistilBERT** â€” Fine-tuned on raw URL text â†’ learns character-level patterns\n",
    "3. **Ensemble** â€” Weighted combination of calibrated probabilities â†’ best of both\n",
    "\n",
    "**Dataset**: [Kaggle Malicious URLs](https://www.kaggle.com/datasets/sid321axn/malicious-urls-dataset) (~651K URLs â†’ binary)\n",
    "\n",
    "**Output**: Packaged models ready for server integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e51cacc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Install Dependencies (run this cell first!)\n",
    "# ============================================================\n",
    "!pip install -q kagglehub xgboost optuna scikit-learn transformers accelerate matplotlib seaborn tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"âœ… All packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf43185",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Š Section 1: Data Loading & Preprocessing\n",
    "Download the dataset, explore class distribution, convert to binary (Safe vs Malicious), and split into train/val/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d596c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# â”€â”€ 1.1 Download Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import kagglehub\n",
    "\n",
    "print(\"ğŸ“¥ Downloading dataset from Kaggle...\")\n",
    "path = kagglehub.dataset_download(\"sid321axn/malicious-urls-dataset\")\n",
    "print(f\"   Downloaded to: {path}\")\n",
    "\n",
    "csv_files = glob.glob(os.path.join(path, \"**\", \"*.csv\"), recursive=True)\n",
    "csv_file = csv_files[0] if csv_files else os.path.join(path, \"malicious_phish.csv\")\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "print(f\"\\nâœ… Loaded {df.shape[0]:,} URLs  |  Columns: {list(df.columns)}\")\n",
    "\n",
    "# â”€â”€ 1.2 Explore â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nğŸ“Š Original class distribution:\")\n",
    "print(df['type'].value_counts().to_string())\n",
    "\n",
    "# â”€â”€ 1.3 Convert to Binary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df['label'] = (df['type'] != 'benign').astype(int)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "df['type'].value_counts().plot.bar(ax=axes[0], color=['#34C759','#FF3B30','#FF9500','#007AFF'])\n",
    "axes[0].set_title('Original 4-Class Distribution'); axes[0].set_ylabel('Count')\n",
    "df['label'].value_counts().rename({0:'Safe', 1:'Malicious'}).plot.bar(ax=axes[1], color=['#34C759','#FF3B30'])\n",
    "axes[1].set_title('Binary (Safe vs Malicious)'); axes[1].set_ylabel('Count')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# â”€â”€ 1.4 Clean â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "before = len(df)\n",
    "df = df.dropna(subset=['url','label']).drop_duplicates(subset=['url'])\n",
    "print(f\"\\nğŸ§¹ Removed {before - len(df):,} rows â†’ {len(df):,} URLs remaining\")\n",
    "\n",
    "# â”€â”€ 1.5 Split: 70 / 15 / 15 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])\n",
    "val_df, test_df   = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
    "\n",
    "print(f\"\\nğŸ“Š Split:\")\n",
    "print(f\"   Train:      {len(train_df):>8,}\")\n",
    "print(f\"   Validation: {len(val_df):>8,}\")\n",
    "print(f\"   Test:       {len(test_df):>8,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e630ab",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸŒ² Section 2: XGBoost with Feature Engineering\n",
    "\n",
    "Hand-crafted **100+ features** covering:\n",
    "- **Length** â€” URL, domain, path, query, subdomain, TLD lengths\n",
    "- **Counts** â€” dots, hyphens, digits, special chars, subdomains, path depth\n",
    "- **Ratios** â€” digit/letter/special proportions, domain-to-URL ratio\n",
    "- **Entropy** â€” Shannon entropy of domain, path, query, subdomain (randomness detection)\n",
    "- **Boolean** â€” IP address, port, HTTPS, hex encoding, punycode, @ symbol\n",
    "- **TLD** â€” suspicious/trusted TLD classification\n",
    "- **Keywords** â€” phishing, malware, brand impersonation detection\n",
    "- **Structure** â€” deep paths, embedded URLs, base64 patterns, shorteners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6337eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from collections import Counter\n",
    "\n",
    "# â”€â”€ Keyword / Pattern Dictionaries â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "SUSPICIOUS_TLDS = frozenset({\n",
    "    \"tk\",\"ml\",\"ga\",\"cf\",\"gq\",\"pw\",\"top\",\"xyz\",\"club\",\"work\",\"click\",\"link\",\n",
    "    \"surf\",\"buzz\",\"fun\",\"monster\",\"quest\",\"cam\",\"icu\",\"cc\",\"ws\",\"info\",\"biz\",\n",
    "    \"su\",\"ru\",\"cn\",\"online\",\"site\",\"website\",\"space\",\"tech\",\"store\",\"stream\",\n",
    "    \"download\",\"win\",\"review\",\"racing\",\"cricket\",\"science\",\"party\",\"gdn\",\n",
    "    \"loan\",\"men\",\"country\",\"kim\",\"date\",\"faith\",\"accountant\",\"bid\",\"trade\",\"webcam\",\n",
    "})\n",
    "TRUSTED_TLDS = frozenset({\n",
    "    \"edu\",\"gov\",\"mil\",\"int\",\"ac.uk\",\"gov.uk\",\"edu.au\",\"gov.au\",\n",
    "})\n",
    "BRAND_KEYWORDS = frozenset({\n",
    "    \"paypal\",\"apple\",\"google\",\"microsoft\",\"amazon\",\"facebook\",\"netflix\",\n",
    "    \"instagram\",\"whatsapp\",\"twitter\",\"linkedin\",\"ebay\",\"dropbox\",\"icloud\",\n",
    "    \"outlook\",\"office365\",\"yahoo\",\"chase\",\"wellsfargo\",\"bankofamerica\",\n",
    "    \"citibank\",\"capitalone\",\"steam\",\"spotify\",\"adobe\",\"coinbase\",\"binance\",\"metamask\",\n",
    "})\n",
    "PHISHING_KEYWORDS = frozenset({\n",
    "    \"login\",\"signin\",\"sign-in\",\"logon\",\"password\",\"verify\",\"verification\",\n",
    "    \"confirm\",\"update\",\"secure\",\"security\",\"account\",\"banking\",\"wallet\",\n",
    "    \"suspend\",\"suspended\",\"urgent\",\"expire\",\"unlock\",\"restore\",\"recover\",\n",
    "    \"validate\",\"authenticate\",\"webscr\",\"customer\",\"support\",\"helpdesk\",\n",
    "})\n",
    "MALWARE_KEYWORDS = frozenset({\n",
    "    \"download\",\"free\",\"crack\",\"keygen\",\"patch\",\"serial\",\"warez\",\"torrent\",\n",
    "    \"nulled\",\"hack\",\"cheat\",\"generator\",\"install\",\"setup\",\"update\",\"flash\",\n",
    "    \"player\",\"codec\",\"driver\",\n",
    "})\n",
    "URL_SHORTENERS = frozenset({\n",
    "    \"bit.ly\",\"goo.gl\",\"tinyurl.com\",\"ow.ly\",\"t.co\",\"is.gd\",\n",
    "    \"buff.ly\",\"adf.ly\",\"j.mp\",\"rb.gy\",\"cutt.ly\",\"tiny.cc\",\n",
    "})\n",
    "DANGEROUS_EXTS = frozenset({\n",
    "    \".exe\",\".dll\",\".bat\",\".cmd\",\".msi\",\".scr\",\".pif\",\".vbs\",\n",
    "    \".js\",\".jar\",\".apk\",\".dmg\",\".zip\",\".rar\",\".7z\",\".iso\",\n",
    "})\n",
    "\n",
    "# â”€â”€ Helper Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def calc_entropy(text: str) -> float:\n",
    "    \"\"\"Shannon entropy â€” higher = more random.\"\"\"\n",
    "    if not text: return 0.0\n",
    "    freq = Counter(text.lower())\n",
    "    n = len(text)\n",
    "    return -sum((c/n) * math.log2(c/n) for c in freq.values() if c > 0)\n",
    "\n",
    "def max_run(text: str, cond) -> int:\n",
    "    \"\"\"Longest consecutive run of chars matching cond.\"\"\"\n",
    "    best = cur = 0\n",
    "    for ch in text:\n",
    "        if cond(ch): cur += 1; best = max(best, cur)\n",
    "        else: cur = 0\n",
    "    return best\n",
    "\n",
    "# â”€â”€ Main Feature Extractor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def extract_features(url: str) -> dict:\n",
    "    \"\"\"Extract 100+ features from a single URL.\"\"\"\n",
    "    f = {}\n",
    "    url = str(url).strip()\n",
    "\n",
    "    try:\n",
    "        parsed = urlparse(url if \"://\" in url else f\"http://{url}\")\n",
    "    except Exception:\n",
    "        return {k: 0 for k in FEATURE_NAMES}\n",
    "\n",
    "    scheme   = parsed.scheme.lower()\n",
    "    netloc   = parsed.netloc.lower()\n",
    "    path     = parsed.path\n",
    "    query    = parsed.query\n",
    "    fragment = parsed.fragment\n",
    "\n",
    "    # Domain cleanup\n",
    "    netloc_no_port = netloc.split(\":\")[0] if (\":\" in netloc and not netloc.startswith(\"[\")) else netloc\n",
    "    domain = netloc_no_port\n",
    "    parts  = domain.split(\".\")\n",
    "    path_parts = [p for p in path.split(\"/\") if p]\n",
    "    subdomain  = \".\".join(parts[:-2]) if len(parts) > 2 else \"\"\n",
    "    tld = parts[-1] if parts else \"\"\n",
    "    if len(parts) >= 3 and parts[-2] in (\"co\",\"com\",\"org\",\"net\",\"gov\",\"ac\",\"edu\"):\n",
    "        tld = f\"{parts[-2]}.{parts[-1]}\"\n",
    "\n",
    "    url_lower  = url.lower()\n",
    "    path_lower = path.lower()\n",
    "\n",
    "    # â•â•â• LENGTH â•â•â•\n",
    "    f['url_length']         = len(url)\n",
    "    f['domain_length']      = len(domain)\n",
    "    f['path_length']        = len(path)\n",
    "    f['query_length']       = len(query)\n",
    "    f['fragment_length']    = len(fragment)\n",
    "    f['subdomain_length']   = len(subdomain)\n",
    "    f['tld_length']         = len(tld)\n",
    "    f['longest_domain_part']= max((len(p) for p in parts), default=0)\n",
    "    f['avg_domain_part_len']= np.mean([len(p) for p in parts]) if parts else 0\n",
    "    f['longest_path_part']  = max((len(p) for p in path_parts), default=0)\n",
    "    f['avg_path_part_len']  = np.mean([len(p) for p in path_parts]) if path_parts else 0\n",
    "\n",
    "    # â•â•â• COUNTS â•â•â•\n",
    "    for ch, name in [(\".\",  \"dot\"),  (\"-\", \"hyphen\"), (\"_\", \"underscore\"),\n",
    "                     (\"/\",  \"slash\"),(\"?\", \"question\"),(\"=\",\"equals\"),\n",
    "                     (\"&\",  \"amp\"),  (\"@\", \"at\"),     (\"%\", \"percent\"),\n",
    "                     (\"~\",  \"tilde\"),(\"#\", \"hash\"),   (\":\", \"colon\"),\n",
    "                     (\";\",  \"semicolon\")]:\n",
    "        f[f'{name}_count'] = url.count(ch)\n",
    "\n",
    "    f['domain_dot_count']    = domain.count(\".\")\n",
    "    f['domain_hyphen_count'] = domain.count(\"-\")\n",
    "    f['domain_digit_count']  = sum(c.isdigit() for c in domain)\n",
    "    f['subdomain_count']     = max(0, len(parts) - 2)\n",
    "    f['path_depth']          = len(path_parts)\n",
    "    f['digit_count']         = sum(c.isdigit() for c in url)\n",
    "    f['letter_count']        = sum(c.isalpha() for c in url)\n",
    "    f['uppercase_count']     = sum(c.isupper() for c in url)\n",
    "    f['special_char_count']  = sum(not c.isalnum() for c in url)\n",
    "\n",
    "    try:\n",
    "        qp = parse_qs(query)\n",
    "        f['query_param_count']     = len(qp)\n",
    "        f['query_value_total_len'] = sum(len(v) for vals in qp.values() for v in vals)\n",
    "    except Exception:\n",
    "        f['query_param_count'] = 0; f['query_value_total_len'] = 0\n",
    "\n",
    "    # â•â•â• RATIOS â•â•â•\n",
    "    ul = max(len(url), 1); dl = max(len(domain), 1)\n",
    "    f['digit_ratio']         = f['digit_count'] / ul\n",
    "    f['letter_ratio']        = f['letter_count'] / ul\n",
    "    f['special_char_ratio']  = f['special_char_count'] / ul\n",
    "    f['uppercase_ratio']     = f['uppercase_count'] / max(f['letter_count'], 1)\n",
    "    f['domain_digit_ratio']  = f['domain_digit_count'] / dl\n",
    "    f['domain_hyphen_ratio'] = f['domain_hyphen_count'] / dl\n",
    "    f['path_url_ratio']      = f['path_length'] / ul\n",
    "    f['query_url_ratio']     = f['query_length'] / ul\n",
    "    f['domain_url_ratio']    = f['domain_length'] / ul\n",
    "\n",
    "    # â•â•â• ENTROPY â•â•â•\n",
    "    f['url_entropy']       = calc_entropy(url)\n",
    "    f['domain_entropy']    = calc_entropy(domain.replace(\".\", \"\"))\n",
    "    f['path_entropy']      = calc_entropy(path)\n",
    "    f['query_entropy']     = calc_entropy(query)\n",
    "    f['subdomain_entropy'] = calc_entropy(subdomain)\n",
    "\n",
    "    # â•â•â• BOOLEAN â•â•â•\n",
    "    f['is_https']                = int(scheme == \"https\")\n",
    "    f['is_http']                 = int(scheme == \"http\")\n",
    "    f['has_www']                 = int(domain.startswith(\"www.\"))\n",
    "    f['has_port']                = int(\":\" in netloc and not netloc.startswith(\"[\"))\n",
    "    f['has_at_symbol']           = int(\"@\" in url)\n",
    "    f['has_double_slash_in_path']= int(\"//\" in path)\n",
    "    f['has_hex_encoding']        = int(bool(re.search(r\"%[0-9a-fA-F]{2}\", url)))\n",
    "    f['has_punycode']            = int(\"xn--\" in domain)\n",
    "    f['has_ip_address']          = int(bool(re.match(r\"^(\\d{1,3}\\.){3}\\d{1,3}$\", domain)))\n",
    "    f['has_hex_ip']              = int(bool(re.match(r\"^(0x[0-9a-f]+\\.){3}0x[0-9a-f]+$\", domain)))\n",
    "    f['has_ip_like']             = int(domain.replace(\".\", \"\").isdigit() and len(domain) > 6)\n",
    "\n",
    "    # â•â•â• TLD â•â•â•\n",
    "    f['is_suspicious_tld'] = int(tld in SUSPICIOUS_TLDS)\n",
    "    f['is_trusted_tld']    = int(tld in TRUSTED_TLDS)\n",
    "    f['is_com']            = int(tld == \"com\")\n",
    "    f['is_org']            = int(tld == \"org\")\n",
    "    f['is_net']            = int(tld == \"net\")\n",
    "    f['is_country_tld']    = int(len(tld) == 2 and tld.isalpha())\n",
    "\n",
    "    # â•â•â• CHARACTER DISTRIBUTION â•â•â•\n",
    "    f['max_consec_digits']  = max_run(url, str.isdigit)\n",
    "    f['max_consec_letters'] = max_run(url, str.isalpha)\n",
    "    f['max_consec_special'] = max_run(url, lambda c: not c.isalnum())\n",
    "    vowels = set(\"aeiou\")\n",
    "    dom_letters = [c for c in domain if c.isalpha()]\n",
    "    f['domain_vowel_ratio'] = sum(c in vowels for c in dom_letters) / max(len(dom_letters), 1)\n",
    "\n",
    "    # â•â•â• KEYWORDS â•â•â•\n",
    "    f['brand_keyword_count']    = sum(1 for b in BRAND_KEYWORDS if b in url_lower)\n",
    "    f['has_brand_in_subdomain'] = int(any(b in subdomain.lower() for b in BRAND_KEYWORDS))\n",
    "    f['phishing_keyword_count'] = sum(1 for k in PHISHING_KEYWORDS if k in url_lower)\n",
    "    f['malware_keyword_count']  = sum(1 for k in MALWARE_KEYWORDS if k in url_lower)\n",
    "    f['is_url_shortener']       = int(any(s in netloc for s in URL_SHORTENERS))\n",
    "    f['has_dangerous_ext']      = int(any(path_lower.endswith(e) for e in DANGEROUS_EXTS))\n",
    "    f['has_exe']                = int(path_lower.endswith(\".exe\"))\n",
    "    f['has_php']                = int(\".php\" in path_lower)\n",
    "\n",
    "    # â•â•â• STRUCTURAL PATTERNS â•â•â•\n",
    "    f['has_double_letters']  = int(bool(re.search(r\"(.)\\1\", domain)))\n",
    "    f['has_long_subdomain']  = int(len(subdomain) > 20)\n",
    "    f['has_deep_path']       = int(len(path_parts) > 5)\n",
    "    f['has_embedded_url']    = int(\"http\" in path_lower or \"www\" in path_lower)\n",
    "    f['has_data_uri']        = int(url_lower.startswith(\"data:\"))\n",
    "    f['has_javascript']      = int(\"javascript:\" in url_lower)\n",
    "    f['has_base64']          = int(bool(re.search(r\"[A-Za-z0-9+/]{20,}={0,2}\", url)))\n",
    "    f['brand_in_domain']     = int(any(b in domain for b in BRAND_KEYWORDS))\n",
    "    f['brand_not_registered']= int(\n",
    "        f['brand_in_domain'] == 1 and not any(\n",
    "            domain == f\"{b}.com\" or domain == f\"www.{b}.com\" for b in BRAND_KEYWORDS\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return f\n",
    "\n",
    "# Build canonical feature name list\n",
    "FEATURE_NAMES = list(extract_features(\"https://www.example.com/path?q=1\").keys())\n",
    "print(f\"âœ… Feature engineering ready â€” {len(FEATURE_NAMES)} features per URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b30e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# â”€â”€ 2.1 Extract Features from All URLs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"âš™ï¸  Extracting features (this takes a few minutes)...\\n\")\n",
    "\n",
    "def extract_batch(urls, desc=\"Extracting\"):\n",
    "    return pd.DataFrame(\n",
    "        [extract_features(str(u)) for u in tqdm(urls, desc=desc)],\n",
    "        columns=FEATURE_NAMES\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "X_train_feat = extract_batch(train_df['url'].tolist(), \"  Train\")\n",
    "X_val_feat   = extract_batch(val_df['url'].tolist(),   \"  Val\")\n",
    "X_test_feat  = extract_batch(test_df['url'].tolist(),  \"  Test\")\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "y_val   = val_df['label'].values\n",
    "y_test  = test_df['label'].values\n",
    "\n",
    "# Class imbalance ratio (used by both XGBoost and DistilBERT later)\n",
    "scale_pos = float((y_train == 0).sum() / (y_train == 1).sum())\n",
    "\n",
    "print(f\"\\nâœ… Feature matrices ready:\")\n",
    "print(f\"   Train: {X_train_feat.shape}\")\n",
    "print(f\"   Val:   {X_val_feat.shape}\")\n",
    "print(f\"   Test:  {X_test_feat.shape}\")\n",
    "print(f\"   Class ratio (safe/malicious): {scale_pos:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a395e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import optuna\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# â”€â”€ 2.2 Hyperparameter Tuning with Optuna (50 trials) â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ğŸ” Running Optuna hyperparameter search...\\n\")\n",
    "\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "print(f\"   GPU available: {USE_GPU}\")\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'device': 'cuda' if USE_GPU else 'cpu',\n",
    "        'tree_method': 'hist',\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': 1500,\n",
    "        'early_stopping_rounds': 50,\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5.0),\n",
    "        'scale_pos_weight': scale_pos,\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0,\n",
    "    }\n",
    "    m = xgb.XGBClassifier(**params)\n",
    "    m.fit(X_train_feat.values, y_train,\n",
    "          eval_set=[(X_val_feat.values, y_val)], verbose=False)\n",
    "    return f1_score(y_val, m.predict(X_val_feat.values))\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nğŸ† Best trial F1: {study.best_value:.4f}\")\n",
    "print(f\"   Best params:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"     {k}: {v}\")\n",
    "\n",
    "# â”€â”€ 2.3 Train Final XGBoost with Best Params â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "bp = study.best_params.copy()\n",
    "bp.update({\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'device': 'cuda' if USE_GPU else 'cpu',\n",
    "    'tree_method': 'hist',\n",
    "    'n_estimators': 3000,          # High ceiling â€” early stopping picks best\n",
    "    'early_stopping_rounds': 100,  # More patience for final model\n",
    "    'scale_pos_weight': scale_pos,\n",
    "    'random_state': 42,\n",
    "    'verbosity': 1,\n",
    "})\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(**bp)\n",
    "xgb_model.fit(X_train_feat.values, y_train,\n",
    "              eval_set=[(X_val_feat.values, y_val)], verbose=True)\n",
    "\n",
    "print(f\"\\n   Best iteration: {xgb_model.best_iteration}\")\n",
    "\n",
    "# â”€â”€ 2.4 Calibrate Probabilities (Platt Scaling) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "xgb_calibrated = CalibratedClassifierCV(xgb_model, method='sigmoid', cv='prefit')\n",
    "xgb_calibrated.fit(X_val_feat.values, y_val)\n",
    "\n",
    "print(\"âœ… XGBoost trained & calibrated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36ca6bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_curve, auc, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score, accuracy_score\n",
    ")\n",
    "\n",
    "# â”€â”€ 2.5 Evaluate XGBoost on Test Set â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "xgb_probs = xgb_calibrated.predict_proba(X_test_feat.values)[:, 1]\n",
    "xgb_preds = (xgb_probs >= 0.5).astype(int)\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"ğŸ“Š XGBoost â€” Test Set Results\")\n",
    "print(\"=\" * 55)\n",
    "print(classification_report(y_test, xgb_preds, target_names=['Safe', 'Malicious']))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, xgb_probs):.4f}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(confusion_matrix(y_test, xgb_preds), annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Safe','Malicious'], yticklabels=['Safe','Malicious'], ax=axes[0])\n",
    "axes[0].set_title('XGBoost â€” Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted'); axes[0].set_ylabel('Actual')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, xgb_probs)\n",
    "axes[1].plot(fpr, tpr, 'b-', linewidth=2, label=f'AUC = {auc(fpr, tpr):.4f}')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "axes[1].set_title('ROC Curve'); axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate'); axes[1].legend()\n",
    "\n",
    "# Feature Importance (Top 20)\n",
    "imp = xgb_model.feature_importances_\n",
    "top20 = np.argsort(imp)[-20:]\n",
    "axes[2].barh(range(20), imp[top20], color='steelblue')\n",
    "axes[2].set_yticks(range(20))\n",
    "axes[2].set_yticklabels([FEATURE_NAMES[i] for i in top20])\n",
    "axes[2].set_title('Top 20 Feature Importances')\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc32979",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ¤– Section 3: DistilBERT Fine-Tuning\n",
    "\n",
    "Fine-tune `distilbert-base-uncased` directly on **raw URL text**.\n",
    "The transformer learns character-level and subword-level patterns that hand-crafted features miss (e.g., subtle typosquatting, obfuscated paths).\n",
    "\n",
    "- **3 epochs** with linear warmup + learning rate decay\n",
    "- **Mixed precision** (FP16) for faster training on GPU\n",
    "- **Class-weighted loss** to handle imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "061aae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "# â”€â”€ 3.1 Dataset & DataLoaders â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ–¥ï¸  Device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "bert_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "class URLDataset(Dataset):\n",
    "    def __init__(self, urls, labels, tokenizer, max_len=128):\n",
    "        self.urls = urls\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.urls)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            str(self.urls[idx]),\n",
    "            truncation=True, padding='max_length',\n",
    "            max_length=self.max_len, return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids':      enc['input_ids'].squeeze(),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(),\n",
    "            'label':          torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "BS = 64\n",
    "train_ds = URLDataset(train_df['url'].tolist(), train_df['label'].tolist(), bert_tokenizer)\n",
    "val_ds   = URLDataset(val_df['url'].tolist(),   val_df['label'].tolist(),   bert_tokenizer)\n",
    "test_ds  = URLDataset(test_df['url'].tolist(),  test_df['label'].tolist(),  bert_tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BS, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BS, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BS, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"\\nâœ… DataLoaders ready:\")\n",
    "print(f\"   Train: {len(train_loader)} batches\")\n",
    "print(f\"   Val:   {len(val_loader)} batches\")\n",
    "print(f\"   Test:  {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec375b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 3.2 Fine-Tune DistilBERT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "\n",
    "bert_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(bert_model.parameters(), lr=LR, weight_decay=0.01)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(total_steps * 0.1),\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda'))\n",
    "\n",
    "# Class-weighted loss for imbalance\n",
    "weights = torch.tensor([1.0, scale_pos], dtype=torch.float).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "best_f1 = 0\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_f1': []}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # â”€â”€ Train â”€â”€\n",
    "    bert_model.train()\n",
    "    t_loss, t_steps = 0.0, 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\")\n",
    "    for batch in pbar:\n",
    "        ids    = batch['input_ids'].to(device)\n",
    "        mask   = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):\n",
    "            out  = bert_model(input_ids=ids, attention_mask=mask)\n",
    "            loss = loss_fn(out.logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(bert_model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        t_loss += loss.item(); t_steps += 1\n",
    "        pbar.set_postfix(loss=f\"{t_loss/t_steps:.4f}\")\n",
    "\n",
    "    # â”€â”€ Validate â”€â”€\n",
    "    bert_model.eval()\n",
    "    v_loss, v_steps = 0.0, 0\n",
    "    preds_all, labels_all = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
    "            ids    = batch['input_ids'].to(device)\n",
    "            mask   = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):\n",
    "                out  = bert_model(input_ids=ids, attention_mask=mask)\n",
    "                loss = loss_fn(out.logits, labels)\n",
    "\n",
    "            v_loss += loss.item(); v_steps += 1\n",
    "            preds_all.extend(torch.argmax(out.logits, 1).cpu().numpy())\n",
    "            labels_all.extend(labels.cpu().numpy())\n",
    "\n",
    "    vf1 = f1_score(labels_all, preds_all)\n",
    "    history['train_loss'].append(t_loss / t_steps)\n",
    "    history['val_loss'].append(v_loss / v_steps)\n",
    "    history['val_f1'].append(vf1)\n",
    "\n",
    "    print(f\"\\n  â†’ train_loss={t_loss/t_steps:.4f}  val_loss={v_loss/v_steps:.4f}  val_f1={vf1:.4f}\")\n",
    "\n",
    "    if vf1 > best_f1:\n",
    "        best_f1 = vf1\n",
    "        torch.save(bert_model.state_dict(), \"best_distilbert.pt\")\n",
    "        print(f\"  ğŸ’¾ Saved best model (F1={vf1:.4f})\")\n",
    "\n",
    "# Reload best checkpoint\n",
    "bert_model.load_state_dict(torch.load(\"best_distilbert.pt\"))\n",
    "print(f\"\\nâœ… DistilBERT training complete! Best Val F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5cc7cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ 3.3 Evaluate DistilBERT on Test Set â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "bert_model.eval()\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"DistilBERT test\"):\n",
    "        ids  = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):\n",
    "            out = bert_model(input_ids=ids, attention_mask=mask)\n",
    "        all_probs.extend(torch.softmax(out.logits, dim=1)[:, 1].cpu().numpy())\n",
    "\n",
    "bert_probs = np.array(all_probs)\n",
    "bert_preds = (bert_probs >= 0.5).astype(int)\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"ğŸ“Š DistilBERT â€” Test Set Results\")\n",
    "print(\"=\" * 55)\n",
    "print(classification_report(y_test, bert_preds, target_names=['Safe', 'Malicious']))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, bert_probs):.4f}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(confusion_matrix(y_test, bert_preds), annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=['Safe','Malicious'], yticklabels=['Safe','Malicious'], ax=axes[0])\n",
    "axes[0].set_title('DistilBERT â€” Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted'); axes[0].set_ylabel('Actual')\n",
    "\n",
    "# Training Curves\n",
    "ax1 = axes[1]\n",
    "ax1.plot(range(1, EPOCHS+1), history['train_loss'], 'b-o', label='Train Loss')\n",
    "ax1.plot(range(1, EPOCHS+1), history['val_loss'],   'r-o', label='Val Loss')\n",
    "ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss'); ax1.set_title('Training Curves')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(1, EPOCHS+1), history['val_f1'], 'g--s', label='Val F1')\n",
    "ax2.set_ylabel('F1 Score'); ax2.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b67032c",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ† Section 4: Ensemble â€” Combining Both Models\n",
    "\n",
    "Weighted average of calibrated probabilities:\n",
    "\n",
    "$$P_{ensemble} = \\alpha \\cdot P_{XGBoost} + (1 - \\alpha) \\cdot P_{DistilBERT}$$\n",
    "\n",
    "The optimal weight $\\alpha$ is found by grid search on the **validation set**, then evaluated on the **test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0450b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4.1 Find Optimal Ensemble Weight (validation set)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# XGBoost validation probabilities\n",
    "xgb_val_p = xgb_calibrated.predict_proba(X_val_feat.values)[:, 1]\n",
    "\n",
    "# DistilBERT validation probabilities\n",
    "bert_model.eval()\n",
    "bvp = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"BERT val probs\"):\n",
    "        ids  = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):\n",
    "            out = bert_model(input_ids=ids, attention_mask=mask)\n",
    "        bvp.extend(torch.softmax(out.logits, dim=1)[:, 1].cpu().numpy())\n",
    "bert_val_p = np.array(bvp)\n",
    "\n",
    "# Grid search for optimal alpha\n",
    "best_alpha, best_ens_f1 = 0.5, 0\n",
    "for a in np.arange(0.0, 1.01, 0.05):\n",
    "    p = ((a * xgb_val_p + (1 - a) * bert_val_p) >= 0.5).astype(int)\n",
    "    f = f1_score(y_val, p)\n",
    "    if f > best_ens_f1:\n",
    "        best_alpha, best_ens_f1 = round(a, 2), f\n",
    "\n",
    "print(f\"ğŸ† Optimal Î± = {best_alpha:.2f}\")\n",
    "print(f\"   XGBoost weight:    {best_alpha:.0%}\")\n",
    "print(f\"   DistilBERT weight: {1-best_alpha:.0%}\")\n",
    "print(f\"   Validation F1:     {best_ens_f1:.4f}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4.2 Final Test Evaluation â€” All 3 Models\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "ens_probs = best_alpha * xgb_probs + (1 - best_alpha) * bert_probs\n",
    "ens_preds = (ens_probs >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ† ENSEMBLE â€” Final Test Set Results\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, ens_preds, target_names=['Safe', 'Malicious']))\n",
    "\n",
    "# ROC + PR curves comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for name, probs, c in [('XGBoost', xgb_probs, 'blue'),\n",
    "                         ('DistilBERT', bert_probs, 'red'),\n",
    "                         ('Ensemble', ens_probs, 'green')]:\n",
    "    fpr, tpr, _ = roc_curve(y_test, probs)\n",
    "    axes[0].plot(fpr, tpr, color=c, linewidth=2, label=f'{name} (AUC={auc(fpr, tpr):.4f})')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "axes[0].set_title('ROC Curves â€” Model Comparison')\n",
    "axes[0].set_xlabel('False Positive Rate'); axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].legend()\n",
    "\n",
    "for name, probs, c in [('XGBoost', xgb_probs, 'blue'),\n",
    "                         ('DistilBERT', bert_probs, 'red'),\n",
    "                         ('Ensemble', ens_probs, 'green')]:\n",
    "    pr, rc, _ = precision_recall_curve(y_test, probs)\n",
    "    ap = average_precision_score(y_test, probs)\n",
    "    axes[1].plot(rc, pr, color=c, linewidth=2, label=f'{name} (AP={ap:.4f})')\n",
    "axes[1].set_title('Precision-Recall Curves')\n",
    "axes[1].set_xlabel('Recall'); axes[1].set_ylabel('Precision')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nğŸ“Š Model Comparison Summary:\")\n",
    "print(\"â”€\" * 55)\n",
    "print(f\"{'Model':<15} {'Accuracy':>10} {'F1':>8} {'ROC-AUC':>10}\")\n",
    "print(\"â”€\" * 55)\n",
    "for name, probs in [('XGBoost', xgb_probs), ('DistilBERT', bert_probs), ('Ensemble', ens_probs)]:\n",
    "    p = (probs >= 0.5).astype(int)\n",
    "    print(f\"{name:<15} {accuracy_score(y_test, p):>10.4f} {f1_score(y_test, p):>8.4f} {roc_auc_score(y_test, probs):>10.4f}\")\n",
    "print(\"â”€\" * 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f678e861",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ’¾ Section 5: Export Models for Server Integration\n",
    "\n",
    "Saves everything needed to run inference on the FastAPI server:\n",
    "- `xgb_model.pkl` â€” Calibrated XGBoost model\n",
    "- `feature_names.json` â€” Feature name list (preserves column order)\n",
    "- `distilbert_url_classifier/` â€” Fine-tuned DistilBERT (tokenizer + weights)\n",
    "- `ensemble_config.json` â€” Ensemble weight Î± + performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9bbc0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, json\n",
    "\n",
    "# â”€â”€ 5.1 Save XGBoost â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with open(\"xgb_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(xgb_calibrated, f)\n",
    "\n",
    "with open(\"feature_names.json\", \"w\") as f:\n",
    "    json.dump(FEATURE_NAMES, f)\n",
    "\n",
    "print(f\"âœ… XGBoost saved: xgb_model.pkl ({len(FEATURE_NAMES)} features)\")\n",
    "\n",
    "# â”€â”€ 5.2 Save DistilBERT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "save_dir = \"distilbert_url_classifier\"\n",
    "bert_model.save_pretrained(save_dir)\n",
    "bert_tokenizer.save_pretrained(save_dir)\n",
    "print(f\"âœ… DistilBERT saved: {save_dir}/\")\n",
    "\n",
    "# â”€â”€ 5.3 Save Ensemble Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "config = {\n",
    "    \"xgb_weight\": float(best_alpha),\n",
    "    \"bert_weight\": float(1 - best_alpha),\n",
    "    \"threshold\": 0.5,\n",
    "    \"n_features\": len(FEATURE_NAMES),\n",
    "    \"xgb_model_file\": \"xgb_model.pkl\",\n",
    "    \"feature_names_file\": \"feature_names.json\",\n",
    "    \"bert_model_dir\": save_dir,\n",
    "    \"metrics\": {\n",
    "        \"xgb_accuracy\":      float(accuracy_score(y_test, xgb_preds)),\n",
    "        \"xgb_f1\":            float(f1_score(y_test, xgb_preds)),\n",
    "        \"xgb_auc\":           float(roc_auc_score(y_test, xgb_probs)),\n",
    "        \"bert_accuracy\":     float(accuracy_score(y_test, bert_preds)),\n",
    "        \"bert_f1\":           float(f1_score(y_test, bert_preds)),\n",
    "        \"bert_auc\":          float(roc_auc_score(y_test, bert_probs)),\n",
    "        \"ensemble_accuracy\": float(accuracy_score(y_test, ens_preds)),\n",
    "        \"ensemble_f1\":       float(f1_score(y_test, ens_preds)),\n",
    "        \"ensemble_auc\":      float(roc_auc_score(y_test, ens_probs)),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(\"ensemble_config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"âœ… Ensemble config saved: ensemble_config.json\")\n",
    "print(json.dumps(config[\"metrics\"], indent=2))\n",
    "\n",
    "# â”€â”€ 5.4 Package & Download â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "!zip -r url_classifier_models.zip \\\n",
    "    xgb_model.pkl \\\n",
    "    feature_names.json \\\n",
    "    distilbert_url_classifier/ \\\n",
    "    ensemble_config.json\n",
    "\n",
    "print(\"\\nğŸ“¦ All models packaged: url_classifier_models.zip\")\n",
    "\n",
    "# Auto-download in Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(\"url_classifier_models.zip\")\n",
    "    print(\"â¬‡ï¸  Download started!\")\n",
    "except ImportError:\n",
    "    print(\"â„¹ï¸  Not running in native Colab browser â€” download the zip manually from the file browser.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
