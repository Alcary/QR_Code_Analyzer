{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6068df59",
   "metadata": {},
   "source": [
    "# URL Security Classifier — Training Pipeline\n",
    "\n",
    "**XGBoost with 95 hand-crafted URL features + Platt calibration + SHAP explainability**\n",
    "\n",
    "### Notebook overview\n",
    "- Trains a URL security classifier using hand-crafted lexical, structural, homograph, and n-gram features.\n",
    "- Uses a calibrated XGBoost model for probability outputs suitable for risk scoring.\n",
    "- Produces SHAP-based explanations for both global and per-sample interpretability.\n",
    "\n",
    "### Pipeline\n",
    "1. Load and merge datasets, normalize URLs, and generate format-diverse safe URL augmentation\n",
    "2. Extract 95 URL features (including homograph and n-gram signals)\n",
    "3. Tune and train XGBoost with Optuna, then apply Platt calibration\n",
    "4. Run SHAP feature-attribution analysis\n",
    "5. Run OOD sanity checks on real-world URL formats\n",
    "6. Export artifacts (`xgb_model.pkl` + `feature_names.json`) for server inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e51cacc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "!pip install -q kagglehub xgboost optuna scikit-learn tldextract shap\n",
    "import warnings; warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf43185",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Data Loading, Normalization, Augmentation & Splitting\n",
    "\n",
    "The original dataset's benign URLs are too simple (e.g., `google.com`, `facebook.com`) and have inconsistent formats (some with schemes, some without, some with `www.`).\n",
    "\n",
    "**Fixes applied:**\n",
    "1. **Normalize** all URLs to have a scheme (`https://`), matching real QR code scanner input.\n",
    "2. **Augment benign class** with ~50K realistic complex safe URLs — but with **format diversity**: randomly varying scheme (`http`/`https`), `www.` prefix (45%/55%), and path presence (bare/simple/complex). This prevents the model from learning spurious formatting rules instead of actual phishing patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d596c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ── 1.1 Download Dataset ─────────────────────────────────────\n",
    "import kagglehub\n",
    "\n",
    "print(\"Downloading dataset from Kaggle...\")\n",
    "path = kagglehub.dataset_download(\"sid321axn/malicious-urls-dataset\")\n",
    "print(f\"   Downloaded to: {path}\")\n",
    "\n",
    "csv_files = glob.glob(os.path.join(path, \"**\", \"*.csv\"), recursive=True)\n",
    "csv_file = csv_files[0] if csv_files else os.path.join(path, \"malicious_phish.csv\")\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "print(f\"\\nLoaded {df.shape[0]:,} URLs  |  Columns: {list(df.columns)}\")\n",
    "\n",
    "# ── 1.2 Explore ──────────────────────────────────────────────\n",
    "print(\"\\nOriginal class distribution:\")\n",
    "print(df['type'].value_counts().to_string())\n",
    "\n",
    "# ── 1.3 Convert to Binary ────────────────────────────────────\n",
    "df['label'] = (df['type'] != 'benign').astype(int)\n",
    "\n",
    "# ── 1.3b Normalize Original URLs ─────────────────────────────\n",
    "# The original dataset has inconsistent formats: bare domains (\"google.com\"),\n",
    "# without schemes (\"www.google.com\"), with http://, etc.\n",
    "# QR codes always produce full URLs.  We normalize everything to have a scheme\n",
    "# so the models learn CONTENT patterns, not formatting artifacts.\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def normalize_url(url):\n",
    "    \"\"\"Ensure every URL has a scheme. Preserve existing schemes.\"\"\"\n",
    "    url = str(url).strip()\n",
    "    if not url:\n",
    "        return url\n",
    "    # Already has a scheme\n",
    "    if '://' in url[:12]:\n",
    "        return url\n",
    "    # Add https:// for bare domains / www. prefixes\n",
    "    return 'https://' + url\n",
    "\n",
    "df['url'] = df['url'].apply(normalize_url)\n",
    "print(f\"\\nNormalized all URLs to have schemes\")\n",
    "print(f\"   Sample: {df['url'].iloc[0]}\")\n",
    "\n",
    "# ── 1.4 Augment Benign URLs with Complex Realistic Examples ──\n",
    "# The original benign set is mostly simple domains (google.com, facebook.com).\n",
    "# Real QR codes point to complex URLs. We augment with synthetic safe URLs.\n",
    "#\n",
    "# CRITICAL: Augmented URLs must be FORMAT-DIVERSE to prevent the model from\n",
    "# learning spurious formatting rules instead of phishing patterns.\n",
    "# Each URL randomly varies: scheme, www prefix, path presence, and structure.\n",
    "\n",
    "SAFE_DOMAINS = [\n",
    "    \"google.com\", \"youtube.com\", \"facebook.com\", \"amazon.com\", \"wikipedia.org\",\n",
    "    \"twitter.com\", \"instagram.com\", \"linkedin.com\", \"reddit.com\", \"netflix.com\",\n",
    "    \"microsoft.com\", \"apple.com\", \"github.com\", \"stackoverflow.com\", \"medium.com\",\n",
    "    \"spotify.com\", \"twitch.tv\", \"ebay.com\", \"cnn.com\", \"bbc.com\",\n",
    "    \"nytimes.com\", \"theguardian.com\", \"reuters.com\", \"walmart.com\", \"target.com\",\n",
    "    \"bestbuy.com\", \"homedepot.com\", \"lowes.com\", \"costco.com\", \"macys.com\",\n",
    "    \"airbnb.com\", \"booking.com\", \"expedia.com\", \"tripadvisor.com\", \"yelp.com\",\n",
    "    \"zillow.com\", \"realtor.com\", \"indeed.com\", \"glassdoor.com\", \"monster.com\",\n",
    "    \"coursera.org\", \"udemy.com\", \"edx.org\", \"khanacademy.org\", \"duolingo.com\",\n",
    "    \"kaggle.com\", \"notion.so\", \"figma.com\", \"canva.com\", \"trello.com\",\n",
    "    \"slack.com\", \"zoom.us\", \"dropbox.com\", \"drive.google.com\", \"docs.google.com\",\n",
    "    \"outlook.com\", \"mail.google.com\", \"icloud.com\", \"proton.me\", \"adobe.com\",\n",
    "    \"salesforce.com\", \"hubspot.com\", \"mailchimp.com\", \"stripe.com\", \"shopify.com\",\n",
    "    \"squarespace.com\", \"wix.com\", \"wordpress.com\", \"blogger.com\", \"tumblr.com\",\n",
    "    \"pinterest.com\", \"tiktok.com\", \"snapchat.com\", \"discord.com\", \"telegram.org\",\n",
    "    \"whatsapp.com\", \"signal.org\", \"paypal.com\", \"venmo.com\", \"robinhood.com\",\n",
    "    \"coinbase.com\", \"binance.com\", \"chase.com\", \"bankofamerica.com\", \"wellsfargo.com\",\n",
    "    \"capitalone.com\", \"fidelity.com\", \"vanguard.com\", \"schwab.com\", \"etrade.com\",\n",
    "    \"hulu.com\", \"disneyplus.com\", \"hbomax.com\", \"peacocktv.com\", \"crunchyroll.com\",\n",
    "    \"imdb.com\", \"rottentomatoes.com\", \"goodreads.com\", \"archive.org\", \"quora.com\",\n",
    "    \"pubmed.ncbi.nlm.nih.gov\", \"scholar.google.com\", \"researchgate.net\", \"jstor.org\",\n",
    "    \"mit.edu\", \"stanford.edu\", \"harvard.edu\", \"ox.ac.uk\", \"cam.ac.uk\",\n",
    "]\n",
    "\n",
    "SAFE_PATH_TEMPLATES = [\n",
    "    \"\",  # ← bare domain (no path) — CRITICAL to include!\n",
    "    \"/\", # ← root path\n",
    "    \"/home\", \"/about\", \"/contact\", \"/help\", \"/faq\", \"/terms\", \"/privacy\",\n",
    "    \"/settings\", \"/profile\", \"/dashboard\", \"/account/settings\",\n",
    "    \"/products/{id}\", \"/items/{id}/details\", \"/search?q={word}&page={n}\",\n",
    "    \"/blog/{year}/{month}/{slug}\", \"/article/{id}\", \"/news/{slug}\",\n",
    "    \"/user/{username}/posts\", \"/user/{username}/profile\",\n",
    "    \"/docs/getting-started\", \"/docs/api/v2/reference\", \"/docs/{section}/{page}\",\n",
    "    \"/en/help/article/{id}\", \"/support/ticket/{id}\",\n",
    "    \"/category/{cat}/subcategory/{sub}\", \"/shop/{cat}?sort=price&order=asc\",\n",
    "    \"/code/{username}/{project}/edit\", \"/code/{username}/{project}/blob/main/src/{file}.py\",\n",
    "    \"/dp/{id}?ref=sr_1_{n}&tag={word}\", \"/gp/product/{id}/ref=ox_sc_act_title_1\",\n",
    "    \"/watch?v={id}&list={id2}&index={n}\", \"/playlist?list={id}\",\n",
    "    \"/r/{subreddit}/comments/{id}/{slug}\", \"/r/{subreddit}/wiki/{page}\",\n",
    "    \"/status/{id}\", \"/i/web/status/{id}\",\n",
    "    \"/maps/place/{place}/@{lat},{lon},{zoom}z\",\n",
    "    \"/flights/results?from={code}&to={code2}&date={date}\",\n",
    "    \"/checkout/cart?item={id}&qty={n}\", \"/order/confirmation/{id}\",\n",
    "    \"/index.php?page={word}&ActiveViewID=tab_{word}\",\n",
    "    \"/app/{hex32}\", \"/document/d/{hex32}/edit\",\n",
    "    \"/spreadsheets/d/{hex32}/edit#gid=0\",\n",
    "    \"/forms/d/{hex32}/viewform\",\n",
    "    \"/meeting/join?meetingId={hex32}\",\n",
    "    \"/{lang}/download/{product}/{version}\",\n",
    "    \"/releases/tag/v{version}\", \"/issues/{n}\", \"/pull/{n}/files\",\n",
    "    \"/datasets/{username}/{dataset}?resource=download\",\n",
    "    \"/competitions/{slug}/leaderboard\", \"/notebooks/{username}/{slug}\",\n",
    "    \"/events/{year}/{slug}/register\",\n",
    "    \"/courses/{slug}/learn/lecture/{n}\",\n",
    "    \"/recipe/{id}/{slug}\",\n",
    "    \"/book/{isbn}\", \"/author/{slug}\",\n",
    "    \"/jobs/{id}/{slug}?utm_source=search&utm_medium=web\",\n",
    "    \"/compare/{productA}-vs-{productB}\",\n",
    "    \"/api/v3/users/{username}/repos?per_page=100&sort=updated\",\n",
    "]\n",
    "\n",
    "def _gen_val(key):\n",
    "    \"\"\"Generate a realistic value for a template placeholder.\"\"\"\n",
    "    if key == \"id\":    return ''.join(random.choices(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\", k=random.randint(6, 12)))\n",
    "    if key == \"id2\":   return ''.join(random.choices(\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_-\", k=random.randint(20, 34)))\n",
    "    if key == \"hex32\": return ''.join(random.choices(\"0123456789abcdef\", k=32))\n",
    "    if key == \"n\":     return str(random.randint(1, 500))\n",
    "    if key in (\"word\", \"slug\", \"cat\", \"sub\", \"section\", \"page\", \"subreddit\",\n",
    "               \"place\", \"product\", \"productA\", \"productB\", \"dataset\", \"lang\"):\n",
    "        words = [\"intro\", \"security\", \"machine-learning\", \"recipes\", \"travel\",\n",
    "                 \"analysis\", \"weather\", \"python\", \"photography\", \"health\",\n",
    "                 \"science\", \"technology\", \"sports\", \"music\", \"gaming\",\n",
    "                 \"finance\", \"education\", \"cooking\", \"fashion\", \"design\"]\n",
    "        return random.choice(words)\n",
    "    if key == \"username\": return ''.join(random.choices(\"abcdefghijklmnopqrstuvwxyz0123456789_\", k=random.randint(5, 15)))\n",
    "    if key == \"file\":   return random.choice([\"main\", \"utils\", \"config\", \"app\", \"index\", \"helpers\"])\n",
    "    if key == \"year\":   return str(random.randint(2020, 2026))\n",
    "    if key == \"month\":  return f\"{random.randint(1,12):02d}\"\n",
    "    if key == \"date\":   return f\"2025-{random.randint(1,12):02d}-{random.randint(1,28):02d}\"\n",
    "    if key == \"lat\":    return f\"{random.uniform(-90, 90):.4f}\"\n",
    "    if key == \"lon\":    return f\"{random.uniform(-180, 180):.4f}\"\n",
    "    if key == \"zoom\":   return str(random.randint(5, 18))\n",
    "    if key in (\"code\", \"code2\"): return ''.join(random.choices(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\", k=3))\n",
    "    if key == \"isbn\":   return ''.join(random.choices(\"0123456789\", k=13))\n",
    "    if key == \"version\": return f\"{random.randint(1,5)}.{random.randint(0,20)}.{random.randint(0,10)}\"\n",
    "    return key\n",
    "\n",
    "def generate_safe_urls(n=50000, seed=42):\n",
    "    \"\"\"\n",
    "    Generate n realistic complex safe URLs with FORMAT DIVERSITY.\n",
    "\n",
    "    Each URL randomly varies across four orthogonal dimensions:\n",
    "      1. Scheme: https:// (85%) vs http:// (15%)\n",
    "      2. www prefix: with www. (45%) vs without (55%)\n",
    "      3. Path: complex path (60%), simple path (25%), bare domain (15%)\n",
    "      4. Query params & fragments (same as before)\n",
    "\n",
    "    This prevents the model from learning spurious formatting rules\n",
    "    (e.g., \"www. = malicious\" or \"no path = malicious\").\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    import re as re_mod\n",
    "    urls = []\n",
    "\n",
    "    for _ in range(n):\n",
    "        domain = random.choice(SAFE_DOMAINS)\n",
    "\n",
    "        # ── Dimension 1: Scheme ──\n",
    "        scheme = \"https\" if random.random() < 0.85 else \"http\"\n",
    "\n",
    "        # ── Dimension 2: www. prefix ──\n",
    "        # Skip www. for domains that already have a subdomain (e.g., drive.google.com)\n",
    "        has_subdomain = domain.count('.') >= 2\n",
    "        if has_subdomain:\n",
    "            host = domain\n",
    "        elif random.random() < 0.45:\n",
    "            host = f\"www.{domain}\"\n",
    "        else:\n",
    "            host = domain\n",
    "\n",
    "        # ── Dimension 3: Path ──\n",
    "        r = random.random()\n",
    "        if r < 0.15:\n",
    "            # 15% — bare domain (no path at all, or just /)\n",
    "            path = random.choice([\"\", \"/\"])\n",
    "        elif r < 0.40:\n",
    "            # 25% — simple short paths\n",
    "            simple_paths = [\"/home\", \"/about\", \"/contact\", \"/help\", \"/faq\",\n",
    "                          \"/terms\", \"/privacy\", \"/settings\", \"/profile\",\n",
    "                          \"/dashboard\", \"/\", \"\"]\n",
    "            path = random.choice(simple_paths)\n",
    "        else:\n",
    "            # 60% — complex path from templates\n",
    "            template = random.choice(SAFE_PATH_TEMPLATES)\n",
    "            path = re_mod.sub(r'\\{(\\w+)\\}', lambda m: _gen_val(m.group(1)), template)\n",
    "\n",
    "        url = f\"{scheme}://{host}{path}\"\n",
    "\n",
    "        # Randomly add extra query params (25% chance, only if no query yet)\n",
    "        if random.random() < 0.25 and \"?\" not in url:\n",
    "            params = [\"utm_source=qr\", \"ref=homepage\", \"lang=en\", \"page=1\",\n",
    "                      \"sort=newest\", \"filter=all\", \"view=grid\", \"tab=overview\"]\n",
    "            url += \"?\" + \"&\".join(random.sample(params, random.randint(1, 3)))\n",
    "\n",
    "        # Randomly add fragment (15% chance)\n",
    "        if random.random() < 0.15:\n",
    "            fragments = [\"top\", \"section-2\", \"overview\", \"comments\", \"reviews\",\n",
    "                         \"pricing\", \"features\", f\"id-{random.randint(1,999)}\"]\n",
    "            url += \"#\" + random.choice(fragments)\n",
    "\n",
    "        urls.append(url)\n",
    "    return urls\n",
    "\n",
    "print(\"Generating synthetic safe URLs (format-diverse)...\")\n",
    "synthetic_safe = generate_safe_urls(50000)\n",
    "synthetic_df = pd.DataFrame({\"url\": synthetic_safe, \"type\": \"benign\", \"label\": 0})\n",
    "\n",
    "# ── Verify format diversity ──\n",
    "n_www = sum(1 for u in synthetic_safe if \"://www.\" in u)\n",
    "n_https = sum(1 for u in synthetic_safe if u.startswith(\"https://\"))\n",
    "n_bare = sum(1 for u in synthetic_safe if u.rstrip('/').count('/') <= 2)\n",
    "print(f\"   Format diversity check:\")\n",
    "print(f\"     www. prefix:  {n_www:>6,} ({n_www/len(synthetic_safe):.0%})\")\n",
    "print(f\"     https scheme: {n_https:>6,} ({n_https/len(synthetic_safe):.0%})\")\n",
    "print(f\"     bare/short:   {n_bare:>6,} ({n_bare/len(synthetic_safe):.0%})\")\n",
    "print(f\"   Samples:\")\n",
    "random.seed(99)\n",
    "for u in random.sample(synthetic_safe, 8):\n",
    "    print(f\"     {u}\")\n",
    "\n",
    "# Merge\n",
    "df = pd.concat([df, synthetic_df], ignore_index=True)\n",
    "print(f\"\\n   Added {len(synthetic_df):,} synthetic safe URLs\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "df['type'].value_counts().plot.bar(ax=axes[0], color=['#34C759','#FF3B30','#FF9500','#007AFF'])\n",
    "axes[0].set_title('Original 4-Class + Augmented Distribution'); axes[0].set_ylabel('Count')\n",
    "df['label'].value_counts().rename({0:'Safe', 1:'Malicious'}).plot.bar(ax=axes[1], color=['#34C759','#FF3B30'])\n",
    "axes[1].set_title('Binary (Safe vs Malicious)'); axes[1].set_ylabel('Count')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ── 1.5 Clean ────────────────────────────────────────────────\n",
    "before = len(df)\n",
    "df = df.dropna(subset=['url','label']).drop_duplicates(subset=['url'])\n",
    "print(f\"\\nRemoved {before - len(df):,} rows -> {len(df):,} URLs remaining\")\n",
    "\n",
    "# ── 1.6 Split: 70 / 15 / 15 ─────────────────────────────────\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])\n",
    "val_df, test_df   = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
    "\n",
    "print(f\"\\nSplit:\")\n",
    "print(f\"   Train:      {len(train_df):>8,}\")\n",
    "print(f\"   Validation: {len(val_df):>8,}\")\n",
    "print(f\"   Test:       {len(test_df):>8,}\")\n",
    "print(f\"\\n   Train label distribution:\")\n",
    "print(f\"     Safe:      {(train_df['label']==0).sum():>8,}\")\n",
    "print(f\"     Malicious: {(train_df['label']==1).sum():>8,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e630ab",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: XGBoost with Feature Engineering (95 features)\n",
    "\n",
    "Hand-crafted **95 features** covering:\n",
    "- **Length** (11) — URL, domain, path, query, subdomain, TLD lengths + averages\n",
    "- **Counts** (24) — dots, hyphens, digits, special chars, subdomains, path depth\n",
    "- **Ratios** (9) — digit/letter/special proportions, domain-to-URL ratio\n",
    "- **Entropy** (5) — Shannon entropy of domain, path, query, subdomain\n",
    "- **Boolean** (11) — IP address, port, HTTPS, hex encoding, punycode, @ symbol\n",
    "- **TLD** (6) — suspicious/trusted TLD classification\n",
    "- **Character** (4) — consecutive runs, vowel ratio\n",
    "- **Keywords** (8) — phishing, malware, brand impersonation\n",
    "- **Structure** (9) — deep paths, embedded URLs, base64, shorteners\n",
    "- **Homograph** (5) — mixed scripts, confusable chars, Levenshtein brand distance, char substitution\n",
    "- **N-gram** (3) — bigram frequency scores for domain/subdomain/path randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6337eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipaddress\n",
    "import re, math\n",
    "from urllib.parse import urlparse, parse_qs, unquote\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "import tldextract\n",
    "\n",
    "# ── Keyword / Pattern Dictionaries ────────────────────────────\n",
    "\n",
    "SUSPICIOUS_TLDS = frozenset({\n",
    "    \"tk\",\"ml\",\"ga\",\"cf\",\"gq\",\"pw\",\"top\",\"xyz\",\"club\",\"work\",\"click\",\"link\",\n",
    "    \"surf\",\"buzz\",\"fun\",\"monster\",\"quest\",\"cam\",\"icu\",\"cc\",\"ws\",\"info\",\"biz\",\n",
    "    \"su\",\"ru\",\"cn\",\"online\",\"site\",\"website\",\"space\",\"tech\",\"store\",\"stream\",\n",
    "    \"download\",\"win\",\"review\",\"racing\",\"cricket\",\"science\",\"party\",\"gdn\",\n",
    "    \"loan\",\"men\",\"country\",\"kim\",\"date\",\"faith\",\"accountant\",\"bid\",\"trade\",\"webcam\",\n",
    "})\n",
    "TRUSTED_TLDS = frozenset({\n",
    "    \"edu\",\"gov\",\"mil\",\"int\",\"ac.uk\",\"gov.uk\",\"edu.au\",\"gov.au\",\n",
    "})\n",
    "BRAND_KEYWORDS = frozenset({\n",
    "    \"paypal\",\"apple\",\"google\",\"microsoft\",\"amazon\",\"facebook\",\"netflix\",\n",
    "    \"instagram\",\"whatsapp\",\"twitter\",\"linkedin\",\"ebay\",\"dropbox\",\"icloud\",\n",
    "    \"outlook\",\"office365\",\"yahoo\",\"chase\",\"wellsfargo\",\"bankofamerica\",\n",
    "    \"citibank\",\"capitalone\",\"steam\",\"spotify\",\"adobe\",\"coinbase\",\"binance\",\"metamask\",\n",
    "})\n",
    "PHISHING_KEYWORDS = frozenset({\n",
    "    \"login\",\"signin\",\"sign-in\",\"logon\",\"password\",\"verify\",\"verification\",\n",
    "    \"confirm\",\"update\",\"secure\",\"security\",\"account\",\"banking\",\"wallet\",\n",
    "    \"suspend\",\"suspended\",\"urgent\",\"expire\",\"unlock\",\"restore\",\"recover\",\n",
    "    \"validate\",\"authenticate\",\"webscr\",\"customer\",\"support\",\"helpdesk\",\n",
    "})\n",
    "MALWARE_KEYWORDS = frozenset({\n",
    "    \"download\",\"free\",\"crack\",\"keygen\",\"patch\",\"serial\",\"warez\",\"torrent\",\n",
    "    \"nulled\",\"hack\",\"cheat\",\"generator\",\"install\",\"setup\",\"update\",\"flash\",\n",
    "    \"player\",\"codec\",\"driver\",\n",
    "})\n",
    "URL_SHORTENERS = frozenset({\n",
    "    \"bit.ly\",\"goo.gl\",\"tinyurl.com\",\"ow.ly\",\"t.co\",\"is.gd\",\n",
    "    \"buff.ly\",\"adf.ly\",\"j.mp\",\"rb.gy\",\"cutt.ly\",\"tiny.cc\",\n",
    "})\n",
    "DANGEROUS_EXTS = frozenset({\n",
    "    \".exe\",\".dll\",\".bat\",\".cmd\",\".msi\",\".scr\",\".pif\",\".vbs\",\n",
    "    \".js\",\".jar\",\".apk\",\".dmg\",\".zip\",\".rar\",\".7z\",\".iso\",\n",
    "})\n",
    "\n",
    "# Common bigrams for domain randomness scoring.\n",
    "# Includes standard English prose bigrams PLUS patterns common in\n",
    "# legitimate domain names (e.g., \"go\", \"oo\", \"ok\", \"bo\", \"ap\", \"eb\").\n",
    "# MUST match the server's url_features.py exactly.\n",
    "_COMMON_BIGRAMS = frozenset({\n",
    "    # Core English prose bigrams\n",
    "    \"th\",\"he\",\"in\",\"er\",\"an\",\"re\",\"on\",\"at\",\"en\",\"nd\",\n",
    "    \"ti\",\"es\",\"or\",\"te\",\"of\",\"ed\",\"is\",\"it\",\"al\",\"ar\",\n",
    "    \"st\",\"to\",\"nt\",\"ng\",\"se\",\"ha\",\"as\",\"ou\",\"io\",\"le\",\n",
    "    \"ve\",\"co\",\"me\",\"de\",\"hi\",\"ri\",\"ro\",\"ic\",\"ne\",\"ea\",\n",
    "    \"ra\",\"ce\",\"li\",\"ch\",\"ll\",\"be\",\"ma\",\"si\",\"om\",\"ur\",\n",
    "    # Domain-typical bigrams (cover common brand names & tech words)\n",
    "    \"go\", \"oo\", \"og\", \"gl\", \"ok\", \"bo\", \"fa\", \"ac\", \"eb\",\n",
    "    \"am\", \"az\", \"ap\", \"pl\", \"pp\", \"tw\", \"et\", \"fl\", \"ix\",\n",
    "    \"pa\", \"sc\", \"ca\", \"op\", \"ub\", \"dr\", \"sp\", \"ot\", \"if\",\n",
    "    \"so\", \"ft\", \"ab\", \"ad\", \"ob\", \"do\", \"ag\", \"gi\", \"ig\",\n",
    "    \"po\", \"pi\", \"cr\", \"ct\", \"di\", \"mi\", \"mo\", \"no\", \"ov\",\n",
    "    \"sh\", \"sk\", \"sl\", \"sn\", \"sw\", \"ta\", \"tr\", \"tu\", \"up\",\n",
    "    \"ut\", \"wa\", \"wi\", \"wo\", \"zo\",\n",
    "})\n",
    "\n",
    "# Homograph / confusable character mappings\n",
    "# MUST match the server's homograph_detector.py\n",
    "CONFUSABLES = {\n",
    "    # Cyrillic -> Latin\n",
    "    '\\u0430': 'a', '\\u0435': 'e', '\\u043e': 'o', '\\u0440': 'p',\n",
    "    '\\u0441': 'c', '\\u0443': 'y', '\\u0445': 'x', '\\u044a': 'b',\n",
    "    '\\u0456': 'i', '\\u0458': 'j', '\\u04bb': 'h', '\\u0501': 'd',\n",
    "    # Greek -> Latin\n",
    "    '\\u03b1': 'a', '\\u03b5': 'e', '\\u03bf': 'o', '\\u03c1': 'p',\n",
    "    '\\u03ba': 'k', '\\u03bd': 'v', '\\u03c4': 't', '\\u03b9': 'i',\n",
    "    # Common number/letter substitutions\n",
    "    '0': 'o', '1': 'l', '!': 'i', '$': 's',\n",
    "    '@': 'a', '3': 'e', '5': 's', '7': 't', '8': 'b',\n",
    "}\n",
    "BRAND_DOMAINS = {\n",
    "    \"google\": \"google.com\", \"facebook\": \"facebook.com\", \"amazon\": \"amazon.com\",\n",
    "    \"apple\": \"apple.com\", \"microsoft\": \"microsoft.com\", \"paypal\": \"paypal.com\",\n",
    "    \"netflix\": \"netflix.com\", \"instagram\": \"instagram.com\", \"twitter\": \"twitter.com\",\n",
    "    \"linkedin\": \"linkedin.com\", \"ebay\": \"ebay.com\", \"dropbox\": \"dropbox.com\",\n",
    "    \"spotify\": \"spotify.com\", \"adobe\": \"adobe.com\", \"yahoo\": \"yahoo.com\",\n",
    "    \"chase\": \"chase.com\", \"wellsfargo\": \"wellsfargo.com\", \"coinbase\": \"coinbase.com\",\n",
    "    \"binance\": \"binance.com\", \"steam\": \"steampowered.com\", \"outlook\": \"outlook.com\",\n",
    "    \"icloud\": \"icloud.com\", \"whatsapp\": \"whatsapp.com\", \"capitalone\": \"capitalone.com\",\n",
    "    \"bankofamerica\": \"bankofamerica.com\", \"citibank\": \"citibank.com\",\n",
    "    \"metamask\": \"metamask.io\", \"slack\": \"slack.com\", \"zoom\": \"zoom.us\",\n",
    "    \"github\": \"github.com\",\n",
    "}\n",
    "\n",
    "# ── Helper Functions ──────────────────────────────────────────\n",
    "\n",
    "def calc_entropy(text):\n",
    "    if not text: return 0.0\n",
    "    freq = Counter(text.lower())\n",
    "    n = len(text)\n",
    "    return -sum((c/n) * math.log2(c/n) for c in freq.values() if c > 0)\n",
    "\n",
    "def max_run(text, cond):\n",
    "    best = cur = 0\n",
    "    for ch in text:\n",
    "        if cond(ch): cur += 1; best = max(best, cur)\n",
    "        else: cur = 0\n",
    "    return best\n",
    "\n",
    "def bigram_score(text):\n",
    "    \"\"\"Fraction of bigrams in common English bigrams.\"\"\"\n",
    "    text = text.lower()\n",
    "    letters = \"\".join(c for c in text if c.isalpha())\n",
    "    if len(letters) < 2: return 0.0\n",
    "    bigrams = [letters[i:i+2] for i in range(len(letters) - 1)]\n",
    "    if not bigrams: return 0.0\n",
    "    return sum(1 for b in bigrams if b in _COMMON_BIGRAMS) / len(bigrams)\n",
    "\n",
    "@lru_cache(maxsize=4096)\n",
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2): return levenshtein_distance(s2, s1)\n",
    "    if len(s2) == 0: return len(s1)\n",
    "    prev = list(range(len(s2) + 1))\n",
    "    for i, c1 in enumerate(s1):\n",
    "        curr = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            curr.append(min(prev[j+1]+1, curr[j]+1, prev[j]+(c1 != c2)))\n",
    "        prev = curr\n",
    "    return prev[-1]\n",
    "\n",
    "def normalize_confusables(text):\n",
    "    return \"\".join(CONFUSABLES.get(c, c) for c in text)\n",
    "\n",
    "def has_mixed_scripts(text):\n",
    "    import unicodedata\n",
    "    scripts = set()\n",
    "    for ch in text:\n",
    "        if ch in \".-_0123456789\":\n",
    "            continue\n",
    "        cat = unicodedata.category(ch)\n",
    "        if cat.startswith(\"L\"):\n",
    "            name = unicodedata.name(ch, \"\").upper()\n",
    "            if \"CYRILLIC\" in name:\n",
    "                scripts.add(\"cyrillic\")\n",
    "            elif \"GREEK\" in name:\n",
    "                scripts.add(\"greek\")\n",
    "            elif \"LATIN\" in name or ch.isascii():\n",
    "                scripts.add(\"latin\")\n",
    "            else:\n",
    "                scripts.add(\"other\")\n",
    "    return int(len(scripts) > 1)\n",
    "\n",
    "def count_confusable_chars(text):\n",
    "    return sum(1 for c in text.lower() if c in CONFUSABLES and not c.isascii())\n",
    "\n",
    "def min_brand_distance(domain):\n",
    "    \"\"\"Minimum Levenshtein distance from domain to any known brand.\"\"\"\n",
    "    clean = domain.lower().lstrip(\"www.\")\n",
    "    normalized = normalize_confusables(clean)\n",
    "\n",
    "    # Use tldextract for accurate domain name extraction\n",
    "    # (handles multi-part TLDs like .co.uk correctly)\n",
    "    ext = tldextract.extract(clean)\n",
    "    domain_name = ext.domain or clean\n",
    "    norm_domain_name = normalize_confusables(domain_name)\n",
    "\n",
    "    min_dist = 999\n",
    "    for brand_key, brand_domain in BRAND_DOMAINS.items():\n",
    "        d1 = levenshtein_distance(domain_name, brand_key)\n",
    "        d2 = levenshtein_distance(norm_domain_name, brand_key)\n",
    "        d3 = levenshtein_distance(clean, brand_domain)\n",
    "        d4 = levenshtein_distance(normalized, brand_domain)\n",
    "        dist = min(d1, d2, d3, d4)\n",
    "        min_dist = min(min_dist, dist)\n",
    "    return min_dist\n",
    "\n",
    "def detect_char_substitution(domain):\n",
    "    \"\"\"Detect leet-speak / character substitution targeting a brand.\"\"\"\n",
    "    # Strip TLD and www using tldextract\n",
    "    ext = tldextract.extract(domain.lower())\n",
    "    name = ext.domain or domain.lower()\n",
    "\n",
    "    # Check if normalizing confusables changes the string AND matches a brand\n",
    "    normalized = normalize_confusables(name)\n",
    "    if normalized != name:\n",
    "        for brand_key in BRAND_DOMAINS:\n",
    "            if brand_key in normalized and brand_key not in name:\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "def extract_homograph_features(domain):\n",
    "    min_dist = min_brand_distance(domain)\n",
    "    clean_domain = domain.lower().rstrip(\".\")\n",
    "    # Exempt official brand domains across all TLDs using tldextract:\n",
    "    # mail.google.co.il -> ext.domain='google' -> in BRAND_DOMAINS -> official\n",
    "    ext = tldextract.extract(clean_domain)\n",
    "    is_official_domain = ext.domain in BRAND_DOMAINS\n",
    "    normalized = normalize_confusables(domain.lower())\n",
    "    is_exact_match = (\n",
    "        any(b in normalized for b in BRAND_DOMAINS)\n",
    "        and not is_official_domain\n",
    "    )\n",
    "    return {\n",
    "        \"homograph_has_mixed_scripts\": has_mixed_scripts(domain),\n",
    "        \"homograph_confusable_chars\": count_confusable_chars(domain),\n",
    "        \"homograph_min_brand_distance\": min_dist,\n",
    "        \"homograph_has_char_sub\": detect_char_substitution(domain),\n",
    "        \"homograph_is_exact_brand\": int(is_exact_match and min_dist <= 2),\n",
    "    }\n",
    "\n",
    "# ── Main Feature Extractor (95 features) ─────────────────────\n",
    "\n",
    "def extract_features(url):\n",
    "    \"\"\"\n",
    "    Extract 95 features from a single URL.\n",
    "    MUST match the server's url_features.py exactly.\n",
    "    \"\"\"\n",
    "    f = {}\n",
    "    url = str(url).strip()\n",
    "\n",
    "    try:\n",
    "        parsed = urlparse(url if \"://\" in url else f\"http://{url}\")\n",
    "    except Exception:\n",
    "        return {k: 0 for k in FEATURE_NAMES}\n",
    "\n",
    "    scheme   = parsed.scheme.lower()\n",
    "    path     = parsed.path\n",
    "    query    = parsed.query\n",
    "    fragment = parsed.fragment\n",
    "\n",
    "    # Use parsed.hostname to correctly handle userinfo URLs\n",
    "    # (e.g. http://user:pass@example.com) where netloc.split(\":\")[0]\n",
    "    # would incorrectly return \"user\" instead of \"example.com\".\n",
    "    domain   = (parsed.hostname or \"\").lower()\n",
    "    try:\n",
    "        has_port = parsed.port is not None\n",
    "    except ValueError:\n",
    "        # Malformed port (non-numeric) — treat as no valid port\n",
    "        has_port = False\n",
    "\n",
    "    parts  = domain.split(\".\")\n",
    "    path_parts = [p for p in path.split(\"/\") if p]\n",
    "\n",
    "    # Use tldextract for accurate subdomain / registered-domain / TLD\n",
    "    # parsing (handles multi-part TLDs like .co.uk, .com.au correctly)\n",
    "    ext = tldextract.extract(domain)\n",
    "    subdomain = ext.subdomain\n",
    "    tld = ext.suffix if ext.suffix else (parts[-1] if parts else \"\")\n",
    "\n",
    "    url_lower  = url.lower()\n",
    "    path_lower = path.lower()\n",
    "\n",
    "    # ═══ LENGTH ═══\n",
    "    f['url_length']         = len(url)\n",
    "    f['domain_length']      = len(domain)\n",
    "    f['path_length']        = len(path)\n",
    "    f['query_length']       = len(query)\n",
    "    f['fragment_length']    = len(fragment)\n",
    "    f['subdomain_length']   = len(subdomain)\n",
    "    f['tld_length']         = len(tld)\n",
    "    f['longest_domain_part']= max((len(p) for p in parts), default=0)\n",
    "    f['avg_domain_part_len']= float(np.mean([len(p) for p in parts])) if parts else 0.0\n",
    "    f['longest_path_part']  = max((len(p) for p in path_parts), default=0)\n",
    "    f['avg_path_part_len']  = float(np.mean([len(p) for p in path_parts])) if path_parts else 0.0\n",
    "\n",
    "    # ═══ COUNTS ═══\n",
    "    for ch, name in [(\".\",  \"dot\"),  (\"-\", \"hyphen\"), (\"_\", \"underscore\"),\n",
    "                     (\"/\",  \"slash\"),(\"?\", \"question\"),(\"=\",\"equals\"),\n",
    "                     (\"&\",  \"amp\"),  (\"@\", \"at\"),     (\"%\", \"percent\"),\n",
    "                     (\"~\",  \"tilde\"),(\"#\", \"hash\"),   (\":\", \"colon\"),\n",
    "                     (\";\",  \"semicolon\")]:\n",
    "        f[f'{name}_count'] = url.count(ch)\n",
    "\n",
    "    f['domain_dot_count']    = domain.count(\".\")\n",
    "    f['domain_hyphen_count'] = domain.count(\"-\")\n",
    "    f['domain_digit_count']  = sum(c.isdigit() for c in domain)\n",
    "    f['subdomain_count']     = subdomain.count(\".\") + 1 if subdomain else 0\n",
    "    f['path_depth']          = len(path_parts)\n",
    "    f['digit_count']         = sum(c.isdigit() for c in url)\n",
    "    f['letter_count']        = sum(c.isalpha() for c in url)\n",
    "    f['uppercase_count']     = sum(c.isupper() for c in url)\n",
    "    f['special_char_count']  = sum(not c.isalnum() for c in url)\n",
    "\n",
    "    try:\n",
    "        qp = parse_qs(query)\n",
    "        f['query_param_count']     = len(qp)\n",
    "        f['query_value_total_len'] = sum(len(v) for vals in qp.values() for v in vals)\n",
    "    except Exception:\n",
    "        f['query_param_count'] = 0; f['query_value_total_len'] = 0\n",
    "\n",
    "    # ═══ RATIOS ═══\n",
    "    ul = max(len(url), 1); dl = max(len(domain), 1)\n",
    "    f['digit_ratio']         = f['digit_count'] / ul\n",
    "    f['letter_ratio']        = f['letter_count'] / ul\n",
    "    f['special_char_ratio']  = f['special_char_count'] / ul\n",
    "    f['uppercase_ratio']     = f['uppercase_count'] / max(f['letter_count'], 1)\n",
    "    f['domain_digit_ratio']  = f['domain_digit_count'] / dl\n",
    "    f['domain_hyphen_ratio'] = f['domain_hyphen_count'] / dl\n",
    "    f['path_url_ratio']      = f['path_length'] / ul\n",
    "    f['query_url_ratio']     = f['query_length'] / ul\n",
    "    f['domain_url_ratio']    = f['domain_length'] / ul\n",
    "\n",
    "    # ═══ ENTROPY ═══\n",
    "    f['url_entropy']       = calc_entropy(url)\n",
    "    f['domain_entropy']    = calc_entropy(domain.replace(\".\", \"\"))\n",
    "    f['path_entropy']      = calc_entropy(path)\n",
    "    f['query_entropy']     = calc_entropy(query)\n",
    "    f['subdomain_entropy'] = calc_entropy(subdomain)\n",
    "\n",
    "    # ═══ BOOLEAN ═══\n",
    "    f['is_https']                = int(scheme == \"https\")\n",
    "    f['is_http']                 = int(scheme == \"http\")\n",
    "    f['has_www']                 = int(domain.startswith(\"www.\"))\n",
    "    f['has_port']                = int(has_port)\n",
    "    f['has_at_symbol']           = int(\"@\" in url)\n",
    "    f['has_double_slash_in_path']= int(\"//\" in path)\n",
    "    f['has_hex_encoding']        = int(unquote(url) != url)\n",
    "    f['has_punycode']            = int(\"xn--\" in domain)\n",
    "    try:\n",
    "        ipaddress.IPv4Address(domain)\n",
    "        f['has_ip_address'] = 1\n",
    "    except ValueError:\n",
    "        f['has_ip_address'] = 0\n",
    "    f['has_hex_ip']              = int(bool(re.match(r\"^(0x[0-9a-f]+\\.){3}0x[0-9a-f]+$\", domain)))\n",
    "    f['has_ip_like']             = int(domain.replace(\".\", \"\").isdigit() and len(domain) > 6)\n",
    "\n",
    "    # ═══ TLD ═══\n",
    "    f['is_suspicious_tld'] = int(tld in SUSPICIOUS_TLDS)\n",
    "    f['is_trusted_tld']    = int(tld in TRUSTED_TLDS)\n",
    "    f['is_com']            = int(tld == \"com\")\n",
    "    f['is_org']            = int(tld == \"org\")\n",
    "    f['is_net']            = int(tld == \"net\")\n",
    "    f['is_country_tld']    = int(len(tld) == 2 and tld.isalpha())\n",
    "\n",
    "    # ═══ CHARACTER DISTRIBUTION ═══\n",
    "    f['max_consec_digits']  = max_run(url, str.isdigit)\n",
    "    f['max_consec_letters'] = max_run(url, str.isalpha)\n",
    "    f['max_consec_special'] = max_run(url, lambda c: not c.isalnum())\n",
    "    vowels = set(\"aeiou\")\n",
    "    dom_letters = [c for c in domain if c.isalpha()]\n",
    "    f['domain_vowel_ratio'] = sum(c in vowels for c in dom_letters) / max(len(dom_letters), 1)\n",
    "\n",
    "    # ═══ KEYWORDS ═══\n",
    "    f['brand_keyword_count']    = sum(1 for b in BRAND_KEYWORDS if b in url_lower)\n",
    "    f['has_brand_in_subdomain'] = int(any(b in subdomain.lower() for b in BRAND_KEYWORDS))\n",
    "    f['phishing_keyword_count'] = sum(1 for k in PHISHING_KEYWORDS if k in url_lower)\n",
    "    f['malware_keyword_count']  = sum(1 for k in MALWARE_KEYWORDS if k in url_lower)\n",
    "    f['is_url_shortener']       = int(ext.registered_domain in URL_SHORTENERS)\n",
    "    f['has_dangerous_ext']      = int(any(path_lower.endswith(e) for e in DANGEROUS_EXTS))\n",
    "    f['has_exe']                = int(path_lower.endswith(\".exe\"))\n",
    "    f['has_php']                = int(\".php\" in path_lower)\n",
    "\n",
    "    # ═══ STRUCTURAL PATTERNS ═══\n",
    "    f['has_double_letters']  = int(bool(re.search(r\"(.)\\1\", domain)))\n",
    "    f['has_long_subdomain']  = int(len(subdomain) > 20)\n",
    "    f['has_deep_path']       = int(len(path_parts) > 5)\n",
    "    f['has_embedded_url']    = int(\"http\" in path_lower or \"www\" in path_lower)\n",
    "    f['has_data_uri']        = int(url_lower.startswith(\"data:\"))\n",
    "    f['has_javascript']      = int(\"javascript:\" in url_lower)\n",
    "    f['has_base64']          = int(bool(re.search(r\"[A-Za-z0-9+/]{20,}={0,2}\", url)))\n",
    "    f['brand_in_domain']     = int(any(b in domain for b in BRAND_KEYWORDS))\n",
    "    # Use ext.domain (registered name without TLD) to correctly\n",
    "    # identify official brand domains across all TLDs.\n",
    "    # e.g. mail.google.co.il -> ext.domain='google' -> not flagged\n",
    "    f['brand_not_registered']= int(\n",
    "        f['brand_in_domain'] == 1\n",
    "        and ext.domain not in BRAND_KEYWORDS\n",
    "    )\n",
    "    )\n",
    "\n",
    "    # ═══ HOMOGRAPH / TYPOSQUATTING ═══\n",
    "    homo = extract_homograph_features(domain)\n",
    "    f['homograph_has_mixed_scripts']  = homo['homograph_has_mixed_scripts']\n",
    "    f['homograph_confusable_chars']   = homo['homograph_confusable_chars']\n",
    "    f['homograph_min_brand_distance'] = homo['homograph_min_brand_distance']\n",
    "    f['homograph_has_char_sub']       = homo['homograph_has_char_sub']\n",
    "    f['homograph_is_exact_brand']     = homo['homograph_is_exact_brand']\n",
    "\n",
    "    # ═══ N-GRAM FEATURES ═══\n",
    "    domain_name_only = ext.domain or domain\n",
    "    f['domain_bigram_score']    = bigram_score(domain_name_only)\n",
    "    f['subdomain_bigram_score'] = bigram_score(subdomain) if subdomain else 0.0\n",
    "    f['path_bigram_score']      = bigram_score(\"\".join(path_parts)) if path_parts else 0.0\n",
    "\n",
    "    return f\n",
    "\n",
    "# Build canonical feature name list\n",
    "FEATURE_NAMES = list(extract_features(\"https://www.example.com/path?q=1\").keys())\n",
    "print(f\"Feature engineering ready — {len(FEATURE_NAMES)} features per URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b30e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# ── 2.1 Extract Features from All URLs ───────────────────────\n",
    "print(\"Extracting 95 features per URL (this takes a few minutes)...\\n\")\n",
    "\n",
    "def extract_batch(urls, desc=\"Extracting\"):\n",
    "    return pd.DataFrame(\n",
    "        [extract_features(str(u)) for u in tqdm(urls, desc=desc)],\n",
    "        columns=FEATURE_NAMES\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "X_train_feat = extract_batch(train_df['url'].tolist(), \"  Train\")\n",
    "X_val_feat   = extract_batch(val_df['url'].tolist(),   \"  Val\")\n",
    "X_test_feat  = extract_batch(test_df['url'].tolist(),  \"  Test\")\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "y_val   = val_df['label'].values\n",
    "y_test  = test_df['label'].values\n",
    "\n",
    "# Class imbalance ratio\n",
    "scale_pos = float((y_train == 0).sum() / (y_train == 1).sum())\n",
    "\n",
    "print(f\"\\nFeature matrices ready:\")\n",
    "print(f\"   Train: {X_train_feat.shape}\")\n",
    "print(f\"   Val:   {X_val_feat.shape}\")\n",
    "print(f\"   Test:  {X_test_feat.shape}\")\n",
    "print(f\"   Class ratio (safe/malicious): {scale_pos:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a395e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# ── 2.2 Hyperparameter Tuning with Optuna (50 trials) ────────\n",
    "print(\"Running Optuna hyperparameter search...\\n\")\n",
    "\n",
    "# Detect GPU via xgboost — no torch dependency needed\n",
    "try:\n",
    "    _test = xgb.XGBClassifier(device='cuda', n_estimators=1)\n",
    "    _test.fit(np.zeros((2, 1)), np.array([0, 1]))\n",
    "    USE_GPU = True\n",
    "except Exception:\n",
    "    USE_GPU = False\n",
    "print(f\"   GPU available: {USE_GPU}\")\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'device': 'cuda' if USE_GPU else 'cpu',\n",
    "        'tree_method': 'hist',\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': 1500,\n",
    "        'early_stopping_rounds': 50,\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5.0),\n",
    "        'scale_pos_weight': scale_pos,\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0,\n",
    "    }\n",
    "    m = xgb.XGBClassifier(**params)\n",
    "    m.fit(X_train_feat.values, y_train,\n",
    "          eval_set=[(X_val_feat.values, y_val)], verbose=False)\n",
    "    return f1_score(y_val, m.predict(X_val_feat.values))\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nBest trial F1: {study.best_value:.4f}\")\n",
    "print(f\"   Best params:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"     {k}: {v}\")\n",
    "\n",
    "# ── 2.3 Train Final XGBoost with Best Params ─────────────────\n",
    "bp = study.best_params.copy()\n",
    "bp.update({\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'device': 'cuda' if USE_GPU else 'cpu',\n",
    "    'tree_method': 'hist',\n",
    "    'n_estimators': 3000,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'scale_pos_weight': scale_pos,\n",
    "    'random_state': 42,\n",
    "    'verbosity': 1,\n",
    "})\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(**bp)\n",
    "xgb_model.fit(X_train_feat.values, y_train,\n",
    "              eval_set=[(X_val_feat.values, y_val)], verbose=True)\n",
    "\n",
    "print(f\"\\n   Best iteration: {xgb_model.best_iteration}\")\n",
    "\n",
    "# ── 2.4 Calibrate Probabilities (Platt Scaling) ──────────────\n",
    "xgb_calibrated = CalibratedClassifierCV(xgb_model, method='sigmoid', cv='prefit')\n",
    "xgb_calibrated.fit(X_val_feat.values, y_val)\n",
    "\n",
    "print(\"XGBoost trained & calibrated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36ca6bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    roc_curve, auc, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score, accuracy_score\n",
    ")\n",
    "\n",
    "# ── 2.5 Evaluate XGBoost on Test Set ─────────────────────────\n",
    "xgb_probs = xgb_calibrated.predict_proba(X_test_feat.values)[:, 1]\n",
    "xgb_preds = (xgb_probs >= 0.5).astype(int)\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"XGBoost — Test Set Results\")\n",
    "print(\"=\" * 55)\n",
    "print(classification_report(y_test, xgb_preds, target_names=['Safe', 'Malicious']))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, xgb_probs):.4f}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Confusion Matrix\n",
    "sns.heatmap(confusion_matrix(y_test, xgb_preds), annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Safe','Malicious'], yticklabels=['Safe','Malicious'], ax=axes[0])\n",
    "axes[0].set_title('XGBoost — Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted'); axes[0].set_ylabel('Actual')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, xgb_probs)\n",
    "axes[1].plot(fpr, tpr, 'b-', linewidth=2, label=f'AUC = {auc(fpr, tpr):.4f}')\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "axes[1].set_title('ROC Curve'); axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate'); axes[1].legend()\n",
    "\n",
    "# Feature Importance (Top 20)\n",
    "imp = xgb_model.feature_importances_\n",
    "top20 = np.argsort(imp)[-20:]\n",
    "axes[2].barh(range(20), imp[top20], color='steelblue')\n",
    "axes[2].set_yticks(range(20))\n",
    "axes[2].set_yticklabels([FEATURE_NAMES[i] for i in top20])\n",
    "axes[2].set_title('Top 20 Feature Importances')\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d901177",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: SHAP Explainability Analysis\n",
    "\n",
    "Use **SHAP (SHapley Additive exPlanations)** to understand which URL features\n",
    "drive XGBoost predictions. TreeExplainer provides exact Shapley values in\n",
    "polynomial time for tree-based models (Lundberg & Lee 2017).\n",
    "\n",
    "Key visualizations:\n",
    "- **Beeswarm plot** — global feature importance + direction of effect\n",
    "- **Individual explanations** — per-URL feature attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15d3420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════\n",
    "# 3.1 SHAP Explainability — XGBoost Feature Attribution\n",
    "# ══════════════════════════════════════════════════════════════\n",
    "import shap\n",
    "\n",
    "# Unwrap calibrated model to get the raw XGBClassifier\n",
    "raw_xgb = xgb_calibrated.calibrated_classifiers_[0].estimator\n",
    "\n",
    "# Create TreeExplainer (exact Shapley values for tree models)\n",
    "explainer = shap.TreeExplainer(raw_xgb)\n",
    "\n",
    "# Compute SHAP values on a sample of the test set (200 samples for speed)\n",
    "sample_idx = np.random.RandomState(42).choice(len(X_test_feat), size=min(200, len(X_test_feat)), replace=False)\n",
    "X_sample = X_test_feat.iloc[sample_idx]\n",
    "shap_values = explainer.shap_values(X_sample.values)\n",
    "\n",
    "# If shap_values is a list (binary classification), take class 1\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values = shap_values[1]\n",
    "\n",
    "# ── Beeswarm Plot (Global Feature Importance) ────────────────\n",
    "print(\"Top Features by Mean |SHAP Value| (Global Importance)\")\n",
    "print(\"=\" * 55)\n",
    "mean_abs = np.abs(shap_values).mean(axis=0)\n",
    "top_idx = np.argsort(mean_abs)[::-1][:15]\n",
    "for i, idx in enumerate(top_idx, 1):\n",
    "    print(f\"  {i:2d}. {FEATURE_NAMES[idx]:<35} |SHAP| = {mean_abs[idx]:.4f}\")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, X_sample, feature_names=FEATURE_NAMES, show=False)\n",
    "plt.title(\"SHAP Beeswarm — XGBoost Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ── Bar Plot (Feature Importance Ranking) ─────────────────────\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, X_sample, feature_names=FEATURE_NAMES,\n",
    "                  plot_type=\"bar\", show=False)\n",
    "plt.title(\"SHAP Bar — Mean |SHAP Value| per Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ── Individual Explanation (one safe, one malicious) ──────────\n",
    "print(\"\\n\" + \"=\" * 55)\n",
    "print(\"Individual SHAP Explanation — Example URLs\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "safe_sample_idx = np.where(y_test[sample_idx] == 0)[0]\n",
    "mal_sample_idx = np.where(y_test[sample_idx] == 1)[0]\n",
    "\n",
    "if len(safe_sample_idx) > 0:\n",
    "    idx = safe_sample_idx[0]\n",
    "    sv = shap_values[idx]\n",
    "    top_k = np.argsort(np.abs(sv))[::-1][:8]\n",
    "    print(f\"\\nSafe URL (sample #{sample_idx[idx]}):\")\n",
    "    for k in top_k:\n",
    "        direction = \"risk ↑\" if sv[k] > 0 else \"safe ↓\"\n",
    "        print(f\"  {FEATURE_NAMES[k]:<35} SHAP={sv[k]:+.4f}  ({direction})\")\n",
    "\n",
    "if len(mal_sample_idx) > 0:\n",
    "    idx = mal_sample_idx[0]\n",
    "    sv = shap_values[idx]\n",
    "    top_k = np.argsort(np.abs(sv))[::-1][:8]\n",
    "    print(f\"\\nMalicious URL (sample #{sample_idx[idx]}):\")\n",
    "    for k in top_k:\n",
    "        direction = \"risk ↑\" if sv[k] > 0 else \"safe ↓\"\n",
    "        print(f\"  {FEATURE_NAMES[k]:<35} SHAP={sv[k]:+.4f}  ({direction})\")\n",
    "\n",
    "print(\"\\nSHAP analysis complete. Feature attributions will be served via API.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f678e861",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: OOD Sanity Check\n",
    "\n",
    "Out-of-distribution sanity check with real-world URLs.\n",
    "Tests FORMAT DIVERSITY: bare domains, www., http://, complex paths.\n",
    "Ensures the model learned actual phishing patterns, not formatting artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9bbc0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ══════════════════════════════════════════════════════════════\n",
    "# 4.1 Out-of-Distribution Sanity Check — XGBoost Only\n",
    "# ══════════════════════════════════════════════════════════════\n",
    "print(\"=\" * 60)\n",
    "print(\"OOD SANITY CHECK — Real-World URLs\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Tests FORMAT DIVERSITY: bare domains, www., http://, complex paths\\n\")\n",
    "\n",
    "ood_urls = [\n",
    "    # ── SAFE: bare domains (the exact format QR scanners produce) ──\n",
    "    (\"https://www.google.com\", \"SAFE\"),\n",
    "    (\"https://www.netflix.com\", \"SAFE\"),\n",
    "    (\"https://www.wikipedia.org\", \"SAFE\"),\n",
    "    (\"https://www.amazon.com\", \"SAFE\"),\n",
    "    (\"https://www.youtube.com\", \"SAFE\"),\n",
    "    (\"https://www.linkedin.com\", \"SAFE\"),\n",
    "    (\"http://www.google.com\", \"SAFE\"),\n",
    "    # ── SAFE: without www ──\n",
    "    (\"https://google.com\", \"SAFE\"),\n",
    "    (\"https://github.com\", \"SAFE\"),\n",
    "    (\"https://reddit.com\", \"SAFE\"),\n",
    "    # ── SAFE: with complex paths ──\n",
    "    (\"https://www.kaggle.com/code/alexandrucalaras/model-training/edit\", \"SAFE\"),\n",
    "    (\"https://x.com/home\", \"SAFE\"),\n",
    "    (\"https://www.amazon.com/dp/B0D5B7TH89/ref=cm_sw_r_cp_ud_dp_abc123\", \"SAFE\"),\n",
    "    (\"https://docs.google.com/spreadsheets/d/1aBcDeFgHiJkLmNoPqRsTuVwXyZ/edit#gid=0\", \"SAFE\"),\n",
    "    (\"https://github.com/facebook/react/pull/28347/files\", \"SAFE\"),\n",
    "    (\"https://www.youtube.com/watch?v=dQw4w9WgXcQ&list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf\", \"SAFE\"),\n",
    "    (\"https://stackoverflow.com/questions/12345678/how-to-parse-json-in-python\", \"SAFE\"),\n",
    "    (\"https://en.wikipedia.org/wiki/Machine_learning#Supervised_learning\", \"SAFE\"),\n",
    "    (\"https://studenti.pub.ro/index.php?page=Informatii&ActiveViewID=tab_infogen\", \"SAFE\"),\n",
    "    (\"https://mail.google.com/mail/u/0/#inbox\", \"SAFE\"),\n",
    "    (\"https://zoom.us/j/1234567890?pwd=aBcDeFgH\", \"SAFE\"),\n",
    "    (\"https://www.reddit.com/r/MachineLearning/comments/abc123/new_paper_on_transformers/\", \"SAFE\"),\n",
    "    # ── MALICIOUS: phishing ──\n",
    "    (\"http://192.168.1.1/login.php?user=admin&redirect=http://evil.com\", \"MALICIOUS\"),\n",
    "    (\"http://g00gle-com.tk/secure/login/verify-account\", \"MALICIOUS\"),\n",
    "    (\"http://free-iphone15-winner-claim.xyz/prize.php?id=28374\", \"MALICIOUS\"),\n",
    "    (\"http://paypa1-security.top/update/billing/confirm.php\", \"MALICIOUS\"),\n",
    "    (\"http://secure-bankofamerica.ml/signin?session=expired\", \"MALICIOUS\"),\n",
    "    (\"http://bit.ly/3xY9z2k\", \"MALICIOUS\"),\n",
    "]\n",
    "\n",
    "print(f\"{'URL':<75} {'Expected':>10} {'XGB':>7} {'Pred':>6}\")\n",
    "print(\"-\" * 105)\n",
    "\n",
    "ood_correct = 0\n",
    "ood_failures = []\n",
    "for url, expected in ood_urls:\n",
    "    feats = pd.DataFrame([extract_features(url)], columns=FEATURE_NAMES).fillna(0).astype(np.float32)\n",
    "    xgb_p = xgb_calibrated.predict_proba(feats.values)[:, 1][0]\n",
    "    pred = \"MAL\" if xgb_p >= 0.5 else \"SAFE\"\n",
    "    correct = (pred == \"SAFE\" and expected == \"SAFE\") or (pred == \"MAL\" and expected == \"MALICIOUS\")\n",
    "    ood_correct += correct\n",
    "    marker = \"OK\" if correct else \"FAIL\"\n",
    "    if not correct:\n",
    "        ood_failures.append((url, expected, xgb_p))\n",
    "\n",
    "    display_url = url[:72] + \"...\" if len(url) > 72 else url\n",
    "    print(f\"{display_url:<75} {expected:>10} {xgb_p:>7.3f} {pred:>5} {marker}\")\n",
    "\n",
    "print(f\"\\nOOD accuracy: {ood_correct}/{len(ood_urls)} ({ood_correct/len(ood_urls):.0%})\")\n",
    "\n",
    "if ood_failures:\n",
    "    print(f\"\\n⚠️  {len(ood_failures)} FAILURES detected — model may have formatting bias!\")\n",
    "    for url, expected, score in ood_failures:\n",
    "        print(f\"  {url[:60]:<60}  expected={expected}  score={score:.3f}\")\n",
    "    print(\"\\nCheck that augmented data includes www., http://, & bare domains.\")\n",
    "else:\n",
    "    print(\"\\n✅ All OOD tests passed — no formatting bias detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7fbe81",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Export Model for Server Integration\n",
    "\n",
    "Saves everything needed to run inference on the FastAPI server:\n",
    "- `xgb_model.pkl` — Calibrated XGBoost model (CalibratedClassifierCV)\n",
    "- `feature_names.json` — Ordered list of 95 feature names\n",
    "\n",
    "Place these files in the server's `models/` directory:\n",
    "```\n",
    "qr-security-server/\n",
    "  models/\n",
    "    xgb_model.pkl\n",
    "    feature_names.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874ece7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "\n",
    "# ── 5.1 Save Calibrated XGBoost Model ────────────────────────\n",
    "joblib.dump(xgb_calibrated, \"xgb_model.pkl\")\n",
    "print(f\"XGBoost saved: xgb_model.pkl\")\n",
    "\n",
    "# ── 5.2 Save Feature Names ───────────────────────────────────\n",
    "with open(\"feature_names.json\", \"w\") as f:\n",
    "    json.dump(FEATURE_NAMES, f)\n",
    "print(f\"Feature names saved: feature_names.json ({len(FEATURE_NAMES)} features)\")\n",
    "\n",
    "# ── 5.3 Summary ──────────────────────────────────────────────\n",
    "print(f\"\\n{'='*55}\")\n",
    "print(\"MODEL EXPORT SUMMARY\")\n",
    "print(f\"{'='*55}\")\n",
    "print(f\"  Model:           CalibratedClassifierCV(XGBClassifier)\")\n",
    "print(f\"  Features:        {len(FEATURE_NAMES)}\")\n",
    "print(f\"  Test Accuracy:   {accuracy_score(y_test, xgb_preds):.4f}\")\n",
    "print(f\"  Test F1:         {f1_score(y_test, xgb_preds):.4f}\")\n",
    "print(f\"  Test ROC-AUC:    {roc_auc_score(y_test, xgb_probs):.4f}\")\n",
    "print(f\"{'='*55}\")\n",
    "\n",
    "# ── 5.4 Package & Download ───────────────────────────────────\n",
    "!zip -r url_classifier_models.zip xgb_model.pkl feature_names.json\n",
    "\n",
    "print(\"\\nAll models packaged: url_classifier_models.zip\")\n",
    "print(\"\\nPlace these files in: qr-security-server/models/\")\n",
    "print(\"  models/xgb_model.pkl\")\n",
    "print(\"  models/feature_names.json\")\n",
    "\n",
    "# Auto-download (Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(\"url_classifier_models.zip\")\n",
    "    print(\"Download started!\")\n",
    "except ImportError:\n",
    "    print(\"Download the zip from the Output tab (Kaggle) or file browser.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}