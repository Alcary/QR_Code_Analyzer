{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6068df59",
   "metadata": {},
   "source": [
    "# ğŸ” URL Security Classifier â€” Model Training\n",
    "\n",
    "## Ensemble: XGBoost + DistilBERT (Binary Classification)\n",
    "\n",
    "**Goal**: Train a high-accuracy model to classify URLs as **Safe** or **Malicious**\n",
    "\n",
    "**Architecture**:\n",
    "1. **XGBoost** â€” 100+ hand-crafted URL features â†’ explainable, fast inference\n",
    "2. **DistilBERT** â€” Fine-tuned on raw URL text â†’ learns character-level patterns\n",
    "3. **Ensemble** â€” Weighted combination of calibrated probabilities â†’ best of both\n",
    "\n",
    "**Dataset**: [Kaggle Malicious URLs](https://www.kaggle.com/datasets/sid321axn/malicious-urls-dataset) (~651K URLs â†’ binary)\n",
    "\n",
    "**Output**: Packaged models ready for server integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51cacc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Install Dependencies (run this cell first!)\n",
    "# ============================================================\n",
    "!pip install -q kagglehub xgboost optuna scikit-learn transformers accelerate matplotlib seaborn tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"âœ… All packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf43185",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Š Section 1: Data Loading & Preprocessing\n",
    "Download the dataset, explore class distribution, convert to binary (Safe vs Malicious), and split into train/val/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d596c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# â”€â”€ 1.1 Download Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import kagglehub\n",
    "\n",
    "print(\"ğŸ“¥ Downloading dataset from Kaggle...\")\n",
    "path = kagglehub.dataset_download(\"sid321axn/malicious-urls-dataset\")\n",
    "print(f\"   Downloaded to: {path}\")\n",
    "\n",
    "csv_files = glob.glob(os.path.join(path, \"**\", \"*.csv\"), recursive=True)\n",
    "csv_file = csv_files[0] if csv_files else os.path.join(path, \"malicious_phish.csv\")\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "print(f\"\\nâœ… Loaded {df.shape[0]:,} URLs  |  Columns: {list(df.columns)}\")\n",
    "\n",
    "# â”€â”€ 1.2 Explore â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nğŸ“Š Original class distribution:\")\n",
    "print(df['type'].value_counts().to_string())\n",
    "\n",
    "# â”€â”€ 1.3 Convert to Binary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df['label'] = (df['type'] != 'benign').astype(int)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "df['type'].value_counts().plot.bar(ax=axes[0], color=['#34C759','#FF3B30','#FF9500','#007AFF'])\n",
    "axes[0].set_title('Original 4-Class Distribution'); axes[0].set_ylabel('Count')\n",
    "df['label'].value_counts().rename({0:'Safe', 1:'Malicious'}).plot.bar(ax=axes[1], color=['#34C759','#FF3B30'])\n",
    "axes[1].set_title('Binary (Safe vs Malicious)'); axes[1].set_ylabel('Count')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# â”€â”€ 1.4 Clean â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "before = len(df)\n",
    "df = df.dropna(subset=['url','label']).drop_duplicates(subset=['url'])\n",
    "print(f\"\\nğŸ§¹ Removed {before - len(df):,} rows â†’ {len(df):,} URLs remaining\")\n",
    "\n",
    "# â”€â”€ 1.5 Split: 70 / 15 / 15 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])\n",
    "val_df, test_df   = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
    "\n",
    "print(f\"\\nğŸ“Š Split:\")\n",
    "print(f\"   Train:      {len(train_df):>8,}\")\n",
    "print(f\"   Validation: {len(val_df):>8,}\")\n",
    "print(f\"   Test:       {len(test_df):>8,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e630ab",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸŒ² Section 2: XGBoost with Feature Engineering\n",
    "\n",
    "Hand-crafted **100+ features** covering:\n",
    "- **Length** â€” URL, domain, path, query, subdomain, TLD lengths\n",
    "- **Counts** â€” dots, hyphens, digits, special chars, subdomains, path depth\n",
    "- **Ratios** â€” digit/letter/special proportions, domain-to-URL ratio\n",
    "- **Entropy** â€” Shannon entropy of domain, path, query, subdomain (randomness detection)\n",
    "- **Boolean** â€” IP address, port, HTTPS, hex encoding, punycode, @ symbol\n",
    "- **TLD** â€” suspicious/trusted TLD classification\n",
    "- **Keywords** â€” phishing, malware, brand impersonation detection\n",
    "- **Structure** â€” deep paths, embedded URLs, base64 patterns, shorteners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6337eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from collections import Counter\n",
    "\n",
    "# â”€â”€ Keyword / Pattern Dictionaries â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "SUSPICIOUS_TLDS = frozenset({\n",
    "    \"tk\",\"ml\",\"ga\",\"cf\",\"gq\",\"pw\",\"top\",\"xyz\",\"club\",\"work\",\"click\",\"link\",\n",
    "    \"surf\",\"buzz\",\"fun\",\"monster\",\"quest\",\"cam\",\"icu\",\"cc\",\"ws\",\"info\",\"biz\",\n",
    "    \"su\",\"ru\",\"cn\",\"online\",\"site\",\"website\",\"space\",\"tech\",\"store\",\"stream\",\n",
    "    \"download\",\"win\",\"review\",\"racing\",\"cricket\",\"science\",\"party\",\"gdn\",\n",
    "    \"loan\",\"men\",\"country\",\"kim\",\"date\",\"faith\",\"accountant\",\"bid\",\"trade\",\"webcam\",\n",
    "})\n",
    "TRUSTED_TLDS = frozenset({\n",
    "    \"edu\",\"gov\",\"mil\",\"int\",\"ac.uk\",\"gov.uk\",\"edu.au\",\"gov.au\",\n",
    "})\n",
    "BRAND_KEYWORDS = frozenset({\n",
    "    \"paypal\",\"apple\",\"google\",\"microsoft\",\"amazon\",\"facebook\",\"netflix\",\n",
    "    \"instagram\",\"whatsapp\",\"twitter\",\"linkedin\",\"ebay\",\"dropbox\",\"icloud\",\n",
    "    \"outlook\",\"office365\",\"yahoo\",\"chase\",\"wellsfargo\",\"bankofamerica\",\n",
    "    \"citibank\",\"capitalone\",\"steam\",\"spotify\",\"adobe\",\"coinbase\",\"binance\",\"metamask\",\n",
    "})\n",
    "PHISHING_KEYWORDS = frozenset({\n",
    "    \"login\",\"signin\",\"sign-in\",\"logon\",\"password\",\"verify\",\"verification\",\n",
    "    \"confirm\",\"update\",\"secure\",\"security\",\"account\",\"banking\",\"wallet\",\n",
    "    \"suspend\",\"suspended\",\"urgent\",\"expire\",\"unlock\",\"restore\",\"recover\",\n",
    "    \"validate\",\"authenticate\",\"webscr\",\"customer\",\"support\",\"helpdesk\",\n",
    "})\n",
    "MALWARE_KEYWORDS = frozenset({\n",
    "    \"download\",\"free\",\"crack\",\"keygen\",\"patch\",\"serial\",\"warez\",\"torrent\",\n",
    "    \"nulled\",\"hack\",\"cheat\",\"generator\",\"install\",\"setup\",\"update\",\"flash\",\n",
    "    \"player\",\"codec\",\"driver\",\n",
    "})\n",
    "URL_SHORTENERS = frozenset({\n",
    "    \"bit.ly\",\"goo.gl\",\"tinyurl.com\",\"ow.ly\",\"t.co\",\"is.gd\",\n",
    "    \"buff.ly\",\"adf.ly\",\"j.mp\",\"rb.gy\",\"cutt.ly\",\"tiny.cc\",\n",
    "})\n",
    "DANGEROUS_EXTS = frozenset({\n",
    "    \".exe\",\".dll\",\".bat\",\".cmd\",\".msi\",\".scr\",\".pif\",\".vbs\",\n",
    "    \".js\",\".jar\",\".apk\",\".dmg\",\".zip\",\".rar\",\".7z\",\".iso\",\n",
    "})\n",
    "\n",
    "# â”€â”€ Helper Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def calc_entropy(text: str) -> float:\n",
    "    \"\"\"Shannon entropy â€” higher = more random.\"\"\"\n",
    "    if not text: return 0.0\n",
    "    freq = Counter(text.lower())\n",
    "    n = len(text)\n",
    "    return -sum((c/n) * math.log2(c/n) for c in freq.values() if c > 0)\n",
    "\n",
    "def max_run(text: str, cond) -> int:\n",
    "    \"\"\"Longest consecutive run of chars matching cond.\"\"\"\n",
    "    best = cur = 0\n",
    "    for ch in text:\n",
    "        if cond(ch): cur += 1; best = max(best, cur)\n",
    "        else: cur = 0\n",
    "    return best\n",
    "\n",
    "# â”€â”€ Main Feature Extractor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "def extract_features(url: str) -> dict:\n",
    "    \"\"\"Extract 100+ features from a single URL.\"\"\"\n",
    "    f = {}\n",
    "    url = str(url).strip()\n",
    "\n",
    "    try:\n",
    "        parsed = urlparse(url if \"://\" in url else f\"http://{url}\")\n",
    "    except Exception:\n",
    "        return {k: 0 for k in FEATURE_NAMES}\n",
    "\n",
    "    scheme   = parsed.scheme.lower()\n",
    "    netloc   = parsed.netloc.lower()\n",
    "    path     = parsed.path\n",
    "    query    = parsed.query\n",
    "    fragment = parsed.fragment\n",
    "\n",
    "    # Domain cleanup\n",
    "    netloc_no_port = netloc.split(\":\")[0] if (\":\" in netloc and not netloc.startswith(\"[\")) else netloc\n",
    "    domain = netloc_no_port\n",
    "    parts  = domain.split(\".\")\n",
    "    path_parts = [p for p in path.split(\"/\") if p]\n",
    "    subdomain  = \".\".join(parts[:-2]) if len(parts) > 2 else \"\"\n",
    "    tld = parts[-1] if parts else \"\"\n",
    "    if len(parts) >= 3 and parts[-2] in (\"co\",\"com\",\"org\",\"net\",\"gov\",\"ac\",\"edu\"):\n",
    "        tld = f\"{parts[-2]}.{parts[-1]}\"\n",
    "\n",
    "    url_lower  = url.lower()\n",
    "    path_lower = path.lower()\n",
    "\n",
    "    # â•â•â• LENGTH â•â•â•\n",
    "    f['url_length']         = len(url)\n",
    "    f['domain_length']      = len(domain)\n",
    "    f['path_length']        = len(path)\n",
    "    f['query_length']       = len(query)\n",
    "    f['fragment_length']    = len(fragment)\n",
    "    f['subdomain_length']   = len(subdomain)\n",
    "    f['tld_length']         = len(tld)\n",
    "    f['longest_domain_part']= max((len(p) for p in parts), default=0)\n",
    "    f['avg_domain_part_len']= np.mean([len(p) for p in parts]) if parts else 0\n",
    "    f['longest_path_part']  = max((len(p) for p in path_parts), default=0)\n",
    "    f['avg_path_part_len']  = np.mean([len(p) for p in path_parts]) if path_parts else 0\n",
    "\n",
    "    # â•â•â• COUNTS â•â•â•\n",
    "    for ch, name in [(\".\",  \"dot\"),  (\"-\", \"hyphen\"), (\"_\", \"underscore\"),\n",
    "                     (\"/\",  \"slash\"),(\"?\", \"question\"),(\"=\",\"equals\"),\n",
    "                     (\"&\",  \"amp\"),  (\"@\", \"at\"),     (\"%\", \"percent\"),\n",
    "                     (\"~\",  \"tilde\"),(\"#\", \"hash\"),   (\":\", \"colon\"),\n",
    "                     (\";\",  \"semicolon\")]:\n",
    "        f[f'{name}_count'] = url.count(ch)\n",
    "\n",
    "    f['domain_dot_count']    = domain.count(\".\")\n",
    "    f['domain_hyphen_count'] = domain.count(\"-\")\n",
    "    f['domain_digit_count']  = sum(c.isdigit() for c in domain)\n",
    "    f['subdomain_count']     = max(0, len(parts) - 2)\n",
    "    f['path_depth']          = len(path_parts)\n",
    "    f['digit_count']         = sum(c.isdigit() for c in url)\n",
    "    f['letter_count']        = sum(c.isalpha() for c in url)\n",
    "    f['uppercase_count']     = sum(c.isupper() for c in url)\n",
    "    f['special_char_count']  = sum(not c.isalnum() for c in url)\n",
    "\n",
    "    try:\n",
    "        qp = parse_qs(query)\n",
    "        f['query_param_count']     = len(qp)\n",
    "        f['query_value_total_len'] = sum(len(v) for vals in qp.values() for v in vals)\n",
    "    except Exception:\n",
    "        f['query_param_count'] = 0; f['query_value_total_len'] = 0\n",
    "\n",
    "    # â•â•â• RATIOS â•â•â•\n",
    "    ul = max(len(url), 1); dl = max(len(domain), 1)\n",
    "    f['digit_ratio']         = f['digit_count'] / ul\n",
    "    f['letter_ratio']        = f['letter_count'] / ul\n",
    "    f['special_char_ratio']  = f['special_char_count'] / ul\n",
    "    f['uppercase_ratio']     = f['uppercase_count'] / max(f['letter_count'], 1)\n",
    "    f['domain_digit_ratio']  = f['domain_digit_count'] / dl\n",
    "    f['domain_hyphen_ratio'] = f['domain_hyphen_count'] / dl\n",
    "    f['path_url_ratio']      = f['path_length'] / ul\n",
    "    f['query_url_ratio']     = f['query_length'] / ul\n",
    "    f['domain_url_ratio']    = f['domain_length'] / ul\n",
    "\n",
    "    # â•â•â• ENTROPY â•â•â•\n",
    "    f['url_entropy']       = calc_entropy(url)\n",
    "    f['domain_entropy']    = calc_entropy(domain.replace(\".\", \"\"))\n",
    "    f['path_entropy']      = calc_entropy(path)\n",
    "    f['query_entropy']     = calc_entropy(query)\n",
    "    f['subdomain_entropy'] = calc_entropy(subdomain)\n",
    "\n",
    "    # â•â•â• BOOLEAN â•â•â•\n",
    "    f['is_https']                = int(scheme == \"https\")\n",
    "    f['is_http']                 = int(scheme == \"http\")\n",
    "    f['has_www']                 = int(domain.startswith(\"www.\"))\n",
    "    f['has_port']                = int(\":\" in netloc and not netloc.startswith(\"[\"))\n",
    "    f['has_at_symbol']           = int(\"@\" in url)\n",
    "    f['has_double_slash_in_path']= int(\"//\" in path)\n",
    "    f['has_hex_encoding']        = int(bool(re.search(r\"%[0-9a-fA-F]{2}\", url)))\n",
    "    f['has_punycode']            = int(\"xn--\" in domain)\n",
    "    f['has_ip_address']          = int(bool(re.match(r\"^(\\d{1,3}\\.){3}\\d{1,3}$\", domain)))\n",
    "    f['has_hex_ip']              = int(bool(re.match(r\"^(0x[0-9a-f]+\\.){3}0x[0-9a-f]+$\", domain)))\n",
    "    f['has_ip_like']             = int(domain.replace(\".\", \"\").isdigit() and len(domain) > 6)\n",
    "\n",
    "    # â•â•â• TLD â•â•â•\n",
    "    f['is_suspicious_tld'] = int(tld in SUSPICIOUS_TLDS)\n",
    "    f['is_trusted_tld']    = int(tld in TRUSTED_TLDS)\n",
    "    f['is_com']            = int(tld == \"com\")\n",
    "    f['is_org']            = int(tld == \"org\")\n",
    "    f['is_net']            = int(tld == \"net\")\n",
    "    f['is_country_tld']    = int(len(tld) == 2 and tld.isalpha())\n",
    "\n",
    "    # â•â•â• CHARACTER DISTRIBUTION â•â•â•\n",
    "    f['max_consec_digits']  = max_run(url, str.isdigit)\n",
    "    f['max_consec_letters'] = max_run(url, str.isalpha)\n",
    "    f['max_consec_special'] = max_run(url, lambda c: not c.isalnum())\n",
    "    vowels = set(\"aeiou\")\n",
    "    dom_letters = [c for c in domain if c.isalpha()]\n",
    "    f['domain_vowel_ratio'] = sum(c in vowels for c in dom_letters) / max(len(dom_letters), 1)\n",
    "\n",
    "    # â•â•â• KEYWORDS â•â•â•\n",
    "    f['brand_keyword_count']    = sum(1 for b in BRAND_KEYWORDS if b in url_lower)\n",
    "    f['has_brand_in_subdomain'] = int(any(b in subdomain.lower() for b in BRAND_KEYWORDS))\n",
    "    f['phishing_keyword_count'] = sum(1 for k in PHISHING_KEYWORDS if k in url_lower)\n",
    "    f['malware_keyword_count']  = sum(1 for k in MALWARE_KEYWORDS if k in url_lower)\n",
    "    f['is_url_shortener']       = int(any(s in netloc for s in URL_SHORTENERS))\n",
    "    f['has_dangerous_ext']      = int(any(path_lower.endswith(e) for e in DANGEROUS_EXTS))\n",
    "    f['has_exe']                = int(path_lower.endswith(\".exe\"))\n",
    "    f['has_php']                = int(\".php\" in path_lower)\n",
    "\n",
    "    # â•â•â• STRUCTURAL PATTERNS â•â•â•\n",
    "    f['has_double_letters']  = int(bool(re.search(r\"(.)\\1\", domain)))\n",
    "    f['has_long_subdomain']  = int(len(subdomain) > 20)\n",
    "    f['has_deep_path']       = int(len(path_parts) > 5)\n",
    "    f['has_embedded_url']    = int(\"http\" in path_lower or \"www\" in path_lower)\n",
    "    f['has_data_uri']        = int(url_lower.startswith(\"data:\"))\n",
    "    f['has_javascript']      = int(\"javascript:\" in url_lower)\n",
    "    f['has_base64']          = int(bool(re.search(r\"[A-Za-z0-9+/]{20,}={0,2}\", url)))\n",
    "    f['brand_in_domain']     = int(any(b in domain for b in BRAND_KEYWORDS))\n",
    "    f['brand_not_registered']= int(\n",
    "        f['brand_in_domain'] == 1 and not any(\n",
    "            domain == f\"{b}.com\" or domain == f\"www.{b}.com\" for b in BRAND_KEYWORDS\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return f\n",
    "\n",
    "# Build canonical feature name list\n",
    "FEATURE_NAMES = list(extract_features(\"https://www.example.com/path?q=1\").keys())\n",
    "print(f\"âœ… Feature engineering ready â€” {len(FEATURE_NAMES)} features per URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b30e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# â”€â”€ 2.1 Extract Features from All URLs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"âš™ï¸  Extracting features (this takes a few minutes)...\\n\")\n",
    "\n",
    "def extract_batch(urls, desc=\"Extracting\"):\n",
    "    return pd.DataFrame(\n",
    "        [extract_features(str(u)) for u in tqdm(urls, desc=desc)],\n",
    "        columns=FEATURE_NAMES\n",
    "    ).fillna(0).astype(np.float32)\n",
    "\n",
    "X_train_feat = extract_batch(train_df['url'].tolist(), \"  Train\")\n",
    "X_val_feat   = extract_batch(val_df['url'].tolist(),   \"  Val\")\n",
    "X_test_feat  = extract_batch(test_df['url'].tolist(),  \"  Test\")\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "y_val   = val_df['label'].values\n",
    "y_test  = test_df['label'].values\n",
    "\n",
    "# Class imbalance ratio (used by both XGBoost and DistilBERT later)\n",
    "scale_pos = float((y_train == 0).sum() / (y_train == 1).sum())\n",
    "\n",
    "print(f\"\\nâœ… Feature matrices ready:\")\n",
    "print(f\"   Train: {X_train_feat.shape}\")\n",
    "print(f\"   Val:   {X_val_feat.shape}\")\n",
    "print(f\"   Test:  {X_test_feat.shape}\")\n",
    "print(f\"   Class ratio (safe/malicious): {scale_pos:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
